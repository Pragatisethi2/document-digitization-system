{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T07:22:47.460819Z",
     "iopub.status.busy": "2025-05-21T07:22:47.460564Z",
     "iopub.status.idle": "2025-05-21T07:24:47.074099Z",
     "shell.execute_reply": "2025-05-21T07:24:47.073368Z",
     "shell.execute_reply.started": "2025-05-21T07:22:47.460793Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.41 s, sys: 403 ms, total: 1.82 s\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install transformers==4.45.0\n",
    "!pip install bitsandbytes==0.44.1 accelerate\n",
    "! pip install einops flash_attn # florence 2\n",
    "!pip install rouge_score\n",
    "\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T07:24:47.075183Z",
     "iopub.status.busy": "2025-05-21T07:24:47.074974Z",
     "iopub.status.idle": "2025-05-21T07:24:47.079620Z",
     "shell.execute_reply": "2025-05-21T07:24:47.078928Z",
     "shell.execute_reply.started": "2025-05-21T07:24:47.075160Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = 'paste your token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T05:59:17.244444Z",
     "iopub.status.busy": "2025-05-21T05:59:17.244109Z",
     "iopub.status.idle": "2025-05-21T05:59:44.451559Z",
     "shell.execute_reply": "2025-05-21T05:59:44.450788Z",
     "shell.execute_reply.started": "2025-05-21T05:59:17.244420Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model: microsoft/Florence-2-base\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on device: cuda:0\n",
      "\n",
      "==================================================\n",
      "Processing Receipts dataset from /kaggle/input/handwritten-data-form-receipts/receipts/receipts\n",
      "==================================================\n",
      "Found 5 image-txt pairs\n",
      "\n",
      "Processing 1/5: image2\n",
      "Loaded image: /kaggle/input/handwritten-data-form-receipts/receipts/receipts/image2.jpg, size: (517, 748)\n",
      "Running OCR extraction...\n",
      "\n",
      "OCR Result Preview:\n",
      "6 January1939M.2s & MartinNo.8Dr. to A Mumphy.2 weeks rent of cottage dhsfrom of 6 to 23 Jan 39@ 357310100Famitary fee10cleaning feeF3-16-0fos deposit...\n",
      "\n",
      "Ground Truth Preview:\n",
      "9th January 1939\n",
      "\n",
      "Mrs E. Martin\n",
      "No. 8\n",
      "Dr. to A. Murphy,\n",
      "\n",
      "2 weeks rent of cottage No. 8\n",
      "from 7th to 21st Jan 39 @ 35/-    3  10  0\n",
      "Sanitary fee        ...\n",
      "\n",
      "Calculating ROUGE scores...\n",
      "ROUGE-1: 0.3248\n",
      "ROUGE-2: 0.1565\n",
      "ROUGE-L: 0.3077\n",
      "\n",
      "Processing 2/5: image1\n",
      "Loaded image: /kaggle/input/handwritten-data-form-receipts/receipts/receipts/image1.jpg, size: (400, 532)\n",
      "Running OCR extraction...\n",
      "\n",
      "OCR Result Preview:\n",
      "Date19A PHOTOGRAPHERS PLACEM133 MERCER STREETNo.NEW YORK N.Y. 10012Reg. No.ClerkACCOUNTFORWARDED123FLORA4(3) PRINTS10657898TOTAL102 8311RETURNS POLICY...\n",
      "\n",
      "Ground Truth Preview:\n",
      "Date 19\n",
      "A PHOTOGRAPHER'S PLACE\n",
      "133 MERCER STREET\n",
      "NEW YORK, N.Y. 10012\n",
      "M No. Clerk Reg. No. \n",
      "\n",
      "1.8/5/98\n",
      "2\n",
      "3 FLORA 85\n",
      "4 (3) PRINTS 10\n",
      "5\n",
      "6\n",
      "7 TAX 7 85\n",
      "8\n",
      "9\n",
      "...\n",
      "\n",
      "Calculating ROUGE scores...\n",
      "ROUGE-1: 0.3676\n",
      "ROUGE-2: 0.2090\n",
      "ROUGE-L: 0.3529\n",
      "\n",
      "Processing 3/5: image4\n",
      "Loaded image: /kaggle/input/handwritten-data-form-receipts/receipts/receipts/image4.png, size: (850, 996)\n",
      "Running OCR extraction...\n",
      "Error during inference: Unable to infer channel dimension format\n",
      "\n",
      "OCR Result Preview:\n",
      "Error: Unable to infer channel dimension format\n",
      "\n",
      "Ground Truth Preview:\n",
      "MEDICAL BILL RECEIPT\n",
      "Receipt Number #14586\n",
      "Date 1011212020\n",
      "Name of Medical Institution: Clinic Yap\n",
      "Practitioner Name: Yap Chang Chui\n",
      "License Number: -...\n",
      "\n",
      "Calculating ROUGE scores...\n",
      "ROUGE-1: 0.0000\n",
      "ROUGE-2: 0.0000\n",
      "ROUGE-L: 0.0000\n",
      "\n",
      "Processing 4/5: image3\n",
      "Loaded image: /kaggle/input/handwritten-data-form-receipts/receipts/receipts/image3.png, size: (924, 601)\n",
      "Running OCR extraction...\n",
      "Error during inference: Unable to infer channel dimension format\n",
      "\n",
      "OCR Result Preview:\n",
      "Error: Unable to infer channel dimension format\n",
      "\n",
      "Ground Truth Preview:\n",
      "RECEIPT\n",
      "\n",
      "Payer: Annie Greens\n",
      "Payee: ABC Furniture Co\n",
      "Address: 5 Any Street\n",
      "Good Suburb\n",
      "Forrest Town X2204Y\n",
      "\n",
      "Address: 101 Tree Store\n",
      "Leafy Vale\n",
      "Forrest...\n",
      "\n",
      "Calculating ROUGE scores...\n",
      "ROUGE-1: 0.0000\n",
      "ROUGE-2: 0.0000\n",
      "ROUGE-L: 0.0000\n",
      "\n",
      "Processing 5/5: image5\n",
      "Loaded image: /kaggle/input/handwritten-data-form-receipts/receipts/receipts/image5.jpg, size: (720, 1203)\n",
      "Running OCR extraction...\n",
      "\n",
      "OCR Result Preview:\n",
      "WILLIAMS' BOOK STOREEstablished 1908708 SOUTH PACIFIC AVENUESan Pedro, Calif. 90731832-36312/15,81DateEllen LeonardNameAddress:123 W IS-1CitySP90732Zi...\n",
      "\n",
      "Ground Truth Preview:\n",
      "WILLIAMS' BOOK STORE\n",
      "Established 1908\n",
      "708 SOUTH PACIFIC AVENUE\n",
      "San Pedro, Calif. 90731\n",
      "832-3631\n",
      "\n",
      "Date 2/21/1981\n",
      "\n",
      "Name Ellen Leonard\n",
      "Address 1351 W 15t...\n",
      "\n",
      "Calculating ROUGE scores...\n",
      "ROUGE-1: 0.2526\n",
      "ROUGE-2: 0.1290\n",
      "ROUGE-L: 0.2526\n",
      "\n",
      "Dataset processing complete. Results saved to ./florence2_results/florence2_receipt_results.json\n",
      "\n",
      "Average ROUGE Scores:\n",
      "  ROUGE-1: 0.1890\n",
      "  ROUGE-2: 0.0989\n",
      "  ROUGE-L: 0.1827\n",
      "Average inference time: 0.90 seconds\n",
      "\n",
      "==================================================\n",
      "Processing Forms dataset from /kaggle/input/handwritten-data-form-receipts/forms/forms\n",
      "==================================================\n",
      "Found 5 image-txt pairs\n",
      "\n",
      "Processing 1/5: image6\n",
      "Loaded image: /kaggle/input/handwritten-data-form-receipts/forms/forms/image6.jpg, size: (846, 1100)\n",
      "Running OCR extraction...\n",
      "\n",
      "OCR Result Preview:\n",
      "Hospital Patient Registration FormPlease fill out the form accurately, All Hygiene is confidential.Full Name:Viett VauthrDate of Birth (D/D/MM/YYYY):2...\n",
      "\n",
      "Ground Truth Preview:\n",
      "Hospital Patient Registration Form\n",
      "Please fill out this form accurately. All information is confidential.\n",
      "\n",
      "Full Name: Virat Vaibhav\n",
      "Date of Birth (DD/...\n",
      "\n",
      "Calculating ROUGE scores...\n",
      "ROUGE-1: 0.4615\n",
      "ROUGE-2: 0.3592\n",
      "ROUGE-L: 0.4615\n",
      "\n",
      "Processing 2/5: image9\n",
      "Loaded image: /kaggle/input/handwritten-data-form-receipts/forms/forms/image9.jpg, size: (830, 1100)\n",
      "Running OCR extraction...\n",
      "\n",
      "OCR Result Preview:\n",
      "Hospital Patient Registration FormPlease fill out this form enquiry. All information is confidential.Full Name:Avant RajputDate of Birth (DD/MM/YYYY):...\n",
      "\n",
      "Ground Truth Preview:\n",
      "Hospital Patient Registration Form\n",
      "Please fill out this form accurately. All information is confidential.\n",
      "\n",
      "Full Name: Anant Rajput\n",
      "Date of Birth (DD/M...\n",
      "\n",
      "Calculating ROUGE scores...\n",
      "ROUGE-1: 0.4926\n",
      "ROUGE-2: 0.3980\n",
      "ROUGE-L: 0.4926\n",
      "\n",
      "Processing 3/5: image8\n",
      "Loaded image: /kaggle/input/handwritten-data-form-receipts/forms/forms/image8.jpg, size: (809, 1100)\n",
      "Running OCR extraction...\n",
      "\n",
      "OCR Result Preview:\n",
      "Hospital Patient Registration FormPlease fill out the form courtesy, All information is confidential.Full Name:Prachi SinghDate of Birth (DD/MM/YYYY):...\n",
      "\n",
      "Ground Truth Preview:\n",
      "Hospital Patient Registration Form\n",
      "Please fill out this form accurately. All information is confidential.\n",
      "\n",
      "Full Name: Prachi Singh\n",
      "Date of Birth (DD/M...\n",
      "\n",
      "Calculating ROUGE scores...\n",
      "ROUGE-1: 0.4851\n",
      "ROUGE-2: 0.3700\n",
      "ROUGE-L: 0.4851\n",
      "\n",
      "Processing 4/5: image10\n",
      "Loaded image: /kaggle/input/handwritten-data-form-receipts/forms/forms/image10.jpg, size: (850, 1091)\n",
      "Running OCR extraction...\n",
      "\n",
      "OCR Result Preview:\n",
      "Hospital Patient Registration FormPlease fill out the form currently, All information is confidential.Full Name:Sherya JainDate of Birth (DD/MM/YYYY):...\n",
      "\n",
      "Ground Truth Preview:\n",
      "Hospital Patient Registration Form\n",
      "Please fill out this form accurately. All information is confidential.\n",
      "\n",
      "Full Name: Shanya Jain\n",
      "Date of Birth (DD/MM...\n",
      "\n",
      "Calculating ROUGE scores...\n",
      "ROUGE-1: 0.4510\n",
      "ROUGE-2: 0.3663\n",
      "ROUGE-L: 0.4510\n",
      "\n",
      "Processing 5/5: image7\n",
      "Loaded image: /kaggle/input/handwritten-data-form-receipts/forms/forms/image7.jpg, size: (850, 1050)\n",
      "Running OCR extraction...\n",
      "\n",
      "OCR Result Preview:\n",
      "Hospital Patient Registration FormPlease all of the form foundry. All information is confidential.Full Name:Karthi BarraDate of Birth (DD/MM/MYYY):04-...\n",
      "\n",
      "Ground Truth Preview:\n",
      "Hospital Patient Registration Form\n",
      "Please fill out this form accurately. All information is confidential.\n",
      "\n",
      "Full Name: Kartik Bairwa\n",
      "Date of Birth (DD/...\n",
      "\n",
      "Calculating ROUGE scores...\n",
      "ROUGE-1: 0.4585\n",
      "ROUGE-2: 0.3547\n",
      "ROUGE-L: 0.4585\n",
      "\n",
      "Dataset processing complete. Results saved to ./florence2_results/florence2_form_results.json\n",
      "\n",
      "Average ROUGE Scores:\n",
      "  ROUGE-1: 0.4698\n",
      "  ROUGE-2: 0.3696\n",
      "  ROUGE-L: 0.4698\n",
      "Average inference time: 2.21 seconds\n",
      "\n",
      "Comparison summary saved to ./florence2_results/florence2_summary.json\n",
      "\n",
      "Dataset Comparison:\n",
      "               Metric Receipts   Forms Better Dataset Difference %\n",
      "0             ROUGE-1   0.1890  0.4698          Forms       59.76%\n",
      "1             ROUGE-2   0.0989  0.3696          Forms       73.24%\n",
      "2             ROUGE-L   0.1827  0.4698          Forms       61.12%\n",
      "3  Inference Time (s)     0.90    2.21       Receipts       59.12%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from rouge_score import rouge_scorer\n",
    "import re\n",
    "import subprocess\n",
    "import psutil\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration class\n",
    "class CFG:\n",
    "    # Model\n",
    "    model_name = 'microsoft/Florence-2-base'\n",
    "    max_tokens = 512\n",
    "    \n",
    "    # Dataset paths\n",
    "    receipt_dir = \"/kaggle/input/handwritten-data-form-receipts/receipts/receipts\"  # Folder with receipt images and txt files\n",
    "    form_dir = \"/kaggle/input/handwritten-data-form-receipts/forms/forms\"        # Folder with form images and txt files\n",
    "    \n",
    "    # Output data\n",
    "    output_dir = \"./florence2_results\"\n",
    "    receipt_output = \"florence2_receipt_results.json\"\n",
    "    form_output = \"florence2_form_results.json\"\n",
    "    summary_output = \"florence2_summary.json\"\n",
    "    \n",
    "    # Image formats to process\n",
    "    img_extensions = ['.jpg', '.jpeg', '.png', '.tif', '.tiff']\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage of the process\"\"\"\n",
    "    try:\n",
    "        process = psutil.Process(os.getpid())\n",
    "        memory_info = process.memory_info()\n",
    "        return {\n",
    "            'rss': memory_info.rss / (1024 * 1024),  # RSS in MB\n",
    "            'vms': memory_info.vms / (1024 * 1024)   # VMS in MB\n",
    "        }\n",
    "    except:\n",
    "        return {'rss': 0, 'vms': 0}\n",
    "\n",
    "def get_cuda_memory_usage():\n",
    "    \"\"\"Get CUDA memory usage using torch\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return {\n",
    "            'allocated': torch.cuda.memory_allocated() / (1024 * 1024),  # MB\n",
    "            'reserved': torch.cuda.memory_reserved() / (1024 * 1024),    # MB\n",
    "            'max_allocated': torch.cuda.max_memory_allocated() / (1024 * 1024)  # MB\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"Build Florence-2 model\"\"\"\n",
    "    print(f'\\nLoading model: {CFG.model_name}\\n')\n",
    "    \n",
    "    # Import required modules\n",
    "    from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "    \n",
    "    try:\n",
    "        # Try to import optional dependencies\n",
    "        subprocess.run([\"pip\", \"install\", \"einops\", \"flash-attn\", \"--quiet\"])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Load the processor\n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        CFG.model_name, \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Load the model without device_map=\"auto\" since it's not supported\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        CFG.model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float32  # Using float32 consistently\n",
    "    ).eval()\n",
    "    # Move model to appropriate device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Model loaded successfully on device: {device}\")\n",
    "    \n",
    "    return processor, model\n",
    "\n",
    "def inference(image, model, processor):\n",
    "    \"\"\"Run inference with Florence-2 model - using OCR task\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Record metrics before inference\n",
    "    metrics_before = {\n",
    "        'memory': get_memory_usage(),\n",
    "        'cuda': get_cuda_memory_usage(),\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Determine if this is Florence model by checking for post_process_generation method\n",
    "        is_florence = hasattr(processor, 'post_process_generation') and callable(getattr(processor, 'post_process_generation'))\n",
    "        \n",
    "        if is_florence:\n",
    "            # Florence-specific way - using OCR task\n",
    "            inputs = processor(\n",
    "                text=\"<OCR>\",  # Simple OCR task\n",
    "                images=image,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Move inputs to device\n",
    "            device = model.device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Run generation\n",
    "            with torch.no_grad():\n",
    "                output_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=CFG.max_tokens,\n",
    "                    do_sample=False\n",
    "                )\n",
    "            \n",
    "            # Process output using Florence-specific method\n",
    "            output_text = processor.decode(output_ids[0], skip_special_tokens=False)\n",
    "            result = processor.post_process_generation(\n",
    "                output_text, \n",
    "                task=\"<OCR>\", \n",
    "                image_size=(image.width, image.height)\n",
    "            )\n",
    "            \n",
    "            # Extract result from OCR task output\n",
    "            if isinstance(result, dict) and '<OCR>' in result:\n",
    "                result = result['<OCR>']\n",
    "        else:\n",
    "            # Florence model should have post_process_generation but just in case\n",
    "            inputs = processor(\n",
    "                text=\"Extract all text from this image.\",\n",
    "                images=image,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Move inputs to device\n",
    "            device = model.device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Run generation\n",
    "            with torch.no_grad():\n",
    "                output_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=CFG.max_tokens,\n",
    "                    do_sample=False\n",
    "                )\n",
    "            \n",
    "            # Generic decoding\n",
    "            result = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {str(e)}\")\n",
    "        result = f\"Error: {str(e)}\"\n",
    "    \n",
    "    # Record metrics after inference\n",
    "    metrics_after = {\n",
    "        'memory': get_memory_usage(),\n",
    "        'cuda': get_cuda_memory_usage(),\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "    \n",
    "    # Calculate runtime\n",
    "    inference_time = metrics_after['timestamp'] - metrics_before['timestamp']\n",
    "    \n",
    "    # Clear CUDA cache\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Return results and metrics\n",
    "    return {\n",
    "        'result': result,\n",
    "        'metrics': {\n",
    "            'before': metrics_before,\n",
    "            'after': metrics_after,\n",
    "            'inference_time': inference_time\n",
    "        }\n",
    "    }\n",
    "\n",
    "def load_annotation_from_txt(txt_path):\n",
    "    \"\"\"Load annotation from a text file\"\"\"\n",
    "    try:\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read().strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading annotation file {txt_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text for better ROUGE matching\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove non-alphanumeric chars except spaces\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    # Trim leading/trailing spaces\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def calculate_rouge_scores(predicted_text, reference_text):\n",
    "    \"\"\"Calculate ROUGE scores between prediction and reference\"\"\"\n",
    "    # Initialize the ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # Normalize texts\n",
    "    normalized_reference = normalize_text(reference_text)\n",
    "    normalized_prediction = normalize_text(predicted_text)\n",
    "    \n",
    "    # Skip if either text is empty\n",
    "    if not normalized_reference or not normalized_prediction:\n",
    "        return {\n",
    "            'rouge1': 0,\n",
    "            'rouge2': 0,\n",
    "            'rougeL': 0,\n",
    "            'debug': {\n",
    "                'reference_empty': not normalized_reference,\n",
    "                'prediction_empty': not normalized_prediction\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Calculate scores\n",
    "    scores = scorer.score(normalized_reference, normalized_prediction)\n",
    "    \n",
    "    # Debug info\n",
    "    debug_info = {\n",
    "        'reference_sample': normalized_reference[:100] + \"...\" if len(normalized_reference) > 100 else normalized_reference,\n",
    "        'prediction_sample': normalized_prediction[:100] + \"...\" if len(normalized_prediction) > 100 else normalized_prediction,\n",
    "        'reference_length': len(normalized_reference),\n",
    "        'prediction_length': len(normalized_prediction)\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'rouge1': scores['rouge1'].fmeasure,\n",
    "        'rouge2': scores['rouge2'].fmeasure,\n",
    "        'rougeL': scores['rougeL'].fmeasure,\n",
    "        'debug': debug_info\n",
    "    }\n",
    "\n",
    "def find_image_txt_pairs(directory):\n",
    "    \"\"\"Find matching image and txt files in a directory\"\"\"\n",
    "    # Get all files\n",
    "    all_files = os.listdir(directory)\n",
    "    \n",
    "    # Find image files\n",
    "    image_files = [f for f in all_files if any(f.lower().endswith(ext) for ext in CFG.img_extensions)]\n",
    "    \n",
    "    # Find matching txt files\n",
    "    pairs = []\n",
    "    for img_file in image_files:\n",
    "        base_name = os.path.splitext(img_file)[0]\n",
    "        txt_file = f\"{base_name}.txt\"\n",
    "        \n",
    "        if txt_file in all_files:\n",
    "            pairs.append({\n",
    "                'image': os.path.join(directory, img_file),\n",
    "                'txt': os.path.join(directory, txt_file),\n",
    "                'base_name': base_name\n",
    "            })\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def process_dataset(dataset_dir, output_file, model, processor, dataset_name):\n",
    "    \"\"\"Process a single dataset (receipts or forms)\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing {dataset_name} dataset from {dataset_dir}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Find image-txt pairs\n",
    "    pairs = find_image_txt_pairs(dataset_dir)\n",
    "    print(f\"Found {len(pairs)} image-txt pairs\")\n",
    "    \n",
    "    if len(pairs) == 0:\n",
    "        print(f\"No valid pairs found in {dataset_dir}\")\n",
    "        return None\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each pair\n",
    "    for i, pair in enumerate(pairs):\n",
    "        print(f\"\\nProcessing {i+1}/{len(pairs)}: {pair['base_name']}\")\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(pair['image'])\n",
    "            print(f\"Loaded image: {pair['image']}, size: {image.size}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {pair['image']}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Load annotation\n",
    "        ground_truth = load_annotation_from_txt(pair['txt'])\n",
    "        if not ground_truth:\n",
    "            print(f\"Empty or invalid annotation file: {pair['txt']}\")\n",
    "            continue\n",
    "        \n",
    "        # Run inference\n",
    "        print(\"Running OCR extraction...\")\n",
    "        ocr_result = inference(image=image, model=model, processor=processor)\n",
    "        \n",
    "        # Print OCR result preview\n",
    "        print(\"\\nOCR Result Preview:\")\n",
    "        preview = str(ocr_result['result'])[:150] + \"...\" if len(str(ocr_result['result'])) > 150 else str(ocr_result['result'])\n",
    "        print(preview)\n",
    "        \n",
    "        # Ground truth preview\n",
    "        print(\"\\nGround Truth Preview:\")\n",
    "        gt_preview = ground_truth[:150] + \"...\" if len(ground_truth) > 150 else ground_truth\n",
    "        print(gt_preview)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        print(\"\\nCalculating ROUGE scores...\")\n",
    "        rouge_scores = calculate_rouge_scores(ocr_result['result'], ground_truth)\n",
    "        \n",
    "        print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "        print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "        print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'filename': pair['base_name'],\n",
    "            'image_path': pair['image'],\n",
    "            'txt_path': pair['txt'],\n",
    "            'ocr_result': str(ocr_result['result']),  # Convert to string in case it's a dict or other type\n",
    "            'ground_truth': ground_truth,\n",
    "            'rouge_scores': rouge_scores,\n",
    "            'inference_time': ocr_result['metrics']['inference_time']\n",
    "        })\n",
    "        \n",
    "        # Save intermediate results\n",
    "        os.makedirs(CFG.output_dir, exist_ok=True)\n",
    "        with open(os.path.join(CFG.output_dir, output_file), 'w') as f:\n",
    "            json.dump({\n",
    "                'model': CFG.model_name,\n",
    "                'dataset': dataset_name,\n",
    "                'results': results\n",
    "            }, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nDataset processing complete. Results saved to {os.path.join(CFG.output_dir, output_file)}\")\n",
    "    \n",
    "    # Calculate average scores\n",
    "    if results:\n",
    "        avg_rouge1 = sum(r['rouge_scores']['rouge1'] for r in results) / len(results)\n",
    "        avg_rouge2 = sum(r['rouge_scores']['rouge2'] for r in results) / len(results)\n",
    "        avg_rougeL = sum(r['rouge_scores']['rougeL'] for r in results) / len(results)\n",
    "        avg_time = sum(r['inference_time'] for r in results) / len(results)\n",
    "        \n",
    "        summary = {\n",
    "            'dataset': dataset_name,\n",
    "            'num_images': len(results),\n",
    "            'avg_rouge1': avg_rouge1,\n",
    "            'avg_rouge2': avg_rouge2,\n",
    "            'avg_rougeL': avg_rougeL,\n",
    "            'avg_inference_time': avg_time\n",
    "        }\n",
    "        \n",
    "        print(\"\\nAverage ROUGE Scores:\")\n",
    "        print(f\"  ROUGE-1: {avg_rouge1:.4f}\")\n",
    "        print(f\"  ROUGE-2: {avg_rouge2:.4f}\")\n",
    "        print(f\"  ROUGE-L: {avg_rougeL:.4f}\")\n",
    "        print(f\"Average inference time: {avg_time:.2f} seconds\")\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    return None\n",
    "\n",
    "def visualize_results(receipt_results, form_results):\n",
    "    \"\"\"Create visualizations comparing performance on both datasets\"\"\"\n",
    "    os.makedirs(CFG.output_dir, exist_ok=True)\n",
    "    \n",
    "    if not receipt_results or not form_results:\n",
    "        print(\"Not enough data to create visualizations\")\n",
    "        return\n",
    "    \n",
    "    # Extract metrics\n",
    "    datasets = ['Receipts', 'Forms']\n",
    "    rouge1_scores = [receipt_results['avg_rouge1'], form_results['avg_rouge1']]\n",
    "    rouge2_scores = [receipt_results['avg_rouge2'], form_results['avg_rouge2']]\n",
    "    rougeL_scores = [receipt_results['avg_rougeL'], form_results['avg_rougeL']]\n",
    "    inf_times = [receipt_results['avg_inference_time'], form_results['avg_inference_time']]\n",
    "    \n",
    "    # Create ROUGE scores comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = np.arange(len(datasets))\n",
    "    width = 0.25\n",
    "    \n",
    "    plt.bar(x - width, rouge1_scores, width, label='ROUGE-1', color='#2196F3')\n",
    "    plt.bar(x, rouge2_scores, width, label='ROUGE-2', color='#4CAF50')\n",
    "    plt.bar(x + width, rougeL_scores, width, label='ROUGE-L', color='#FFC107')\n",
    "    \n",
    "    plt.xlabel('Datasets')\n",
    "    plt.ylabel('ROUGE Score')\n",
    "    plt.title('Florence-2 Performance on Different Datasets')\n",
    "    plt.xticks(x, datasets)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for i, model_idx in enumerate(x):\n",
    "        plt.text(model_idx - width, rouge1_scores[i] + 0.02, f\"{rouge1_scores[i]:.3f}\", \n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "        plt.text(model_idx, rouge2_scores[i] + 0.02, f\"{rouge2_scores[i]:.3f}\", \n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "        plt.text(model_idx + width, rougeL_scores[i] + 0.02, f\"{rougeL_scores[i]:.3f}\", \n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CFG.output_dir, 'dataset_comparison.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Create inference time comparison\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(datasets, inf_times, color=['#2196F3', '#4CAF50'])\n",
    "    plt.xlabel('Datasets')\n",
    "    plt.ylabel('Average Inference Time (seconds)')\n",
    "    plt.title('Inference Time Comparison')\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for i, time in enumerate(inf_times):\n",
    "        plt.text(i, time + 0.1, f\"{time:.2f}s\", ha='center', va='bottom')\n",
    "    \n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CFG.output_dir, 'inference_time_comparison.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Return file paths for reporting\n",
    "    return {\n",
    "        'rouge_comparison': os.path.join(CFG.output_dir, 'dataset_comparison.png'),\n",
    "        'time_comparison': os.path.join(CFG.output_dir, 'inference_time_comparison.png')\n",
    "    }\n",
    "\n",
    "def create_comparison_table(receipt_summary, form_summary):\n",
    "    \"\"\"Create a comparison table for the two datasets\"\"\"\n",
    "    if not receipt_summary or not form_summary:\n",
    "        return None\n",
    "    \n",
    "    # Determine which dataset performs better for each metric\n",
    "    better_dataset = {\n",
    "        'rouge1': 'Receipts' if receipt_summary['avg_rouge1'] > form_summary['avg_rouge1'] else 'Forms',\n",
    "        'rouge2': 'Receipts' if receipt_summary['avg_rouge2'] > form_summary['avg_rouge2'] else 'Forms',\n",
    "        'rougeL': 'Receipts' if receipt_summary['avg_rougeL'] > form_summary['avg_rougeL'] else 'Forms',\n",
    "        'time': 'Receipts' if receipt_summary['avg_inference_time'] < form_summary['avg_inference_time'] else 'Forms'\n",
    "    }\n",
    "    \n",
    "    # Calculate percentage difference\n",
    "    diff_percent = {\n",
    "        'rouge1': abs(receipt_summary['avg_rouge1'] - form_summary['avg_rouge1']) / \n",
    "                 max(receipt_summary['avg_rouge1'], form_summary['avg_rouge1']) * 100,\n",
    "        'rouge2': abs(receipt_summary['avg_rouge2'] - form_summary['avg_rouge2']) / \n",
    "                 max(receipt_summary['avg_rouge2'], form_summary['avg_rouge2']) * 100,\n",
    "        'rougeL': abs(receipt_summary['avg_rougeL'] - form_summary['avg_rougeL']) / \n",
    "                 max(receipt_summary['avg_rougeL'], form_summary['avg_rougeL']) * 100,\n",
    "        'time': abs(receipt_summary['avg_inference_time'] - form_summary['avg_inference_time']) / \n",
    "               max(receipt_summary['avg_inference_time'], form_summary['avg_inference_time']) * 100\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame for the table\n",
    "    df = pd.DataFrame({\n",
    "        'Metric': ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'Inference Time (s)'],\n",
    "        'Receipts': [\n",
    "            f\"{receipt_summary['avg_rouge1']:.4f}\",\n",
    "            f\"{receipt_summary['avg_rouge2']:.4f}\",\n",
    "            f\"{receipt_summary['avg_rougeL']:.4f}\",\n",
    "            f\"{receipt_summary['avg_inference_time']:.2f}\"\n",
    "        ],\n",
    "        'Forms': [\n",
    "            f\"{form_summary['avg_rouge1']:.4f}\",\n",
    "            f\"{form_summary['avg_rouge2']:.4f}\",\n",
    "            f\"{form_summary['avg_rougeL']:.4f}\",\n",
    "            f\"{form_summary['avg_inference_time']:.2f}\"\n",
    "        ],\n",
    "        'Better Dataset': [\n",
    "            better_dataset['rouge1'],\n",
    "            better_dataset['rouge2'],\n",
    "            better_dataset['rougeL'],\n",
    "            better_dataset['time']\n",
    "        ],\n",
    "        'Difference %': [\n",
    "            f\"{diff_percent['rouge1']:.2f}%\",\n",
    "            f\"{diff_percent['rouge2']:.2f}%\",\n",
    "            f\"{diff_percent['rougeL']:.2f}%\",\n",
    "            f\"{diff_percent['time']:.2f}%\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_path = os.path.join(CFG.output_dir, 'dataset_comparison.csv')\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Also save as markdown table\n",
    "    md_table = df.to_markdown(index=False)\n",
    "    with open(os.path.join(CFG.output_dir, 'dataset_comparison.md'), 'w') as f:\n",
    "        f.write(md_table)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # Create output directory\n",
    "    os.makedirs(CFG.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Build model once\n",
    "    processor, model = build_model()\n",
    "    \n",
    "    # Process receipt dataset\n",
    "    receipt_summary = process_dataset(\n",
    "        dataset_dir=CFG.receipt_dir,\n",
    "        output_file=CFG.receipt_output,\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        dataset_name=\"Receipts\"\n",
    "    )\n",
    "    \n",
    "    # Process form dataset\n",
    "    form_summary = process_dataset(\n",
    "        dataset_dir=CFG.form_dir,\n",
    "        output_file=CFG.form_output,\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        dataset_name=\"Forms\"\n",
    "    )\n",
    "    \n",
    "    # Create visualizations\n",
    "    if receipt_summary and form_summary:\n",
    "        chart_paths = visualize_results(receipt_summary, form_summary)\n",
    "        \n",
    "        # Create comparison table\n",
    "        comparison_table = create_comparison_table(receipt_summary, form_summary)\n",
    "        \n",
    "        # Save combined summary\n",
    "        combined_summary = {\n",
    "            'model': CFG.model_name,\n",
    "            'receipts': receipt_summary,\n",
    "            'forms': form_summary,\n",
    "            'charts': chart_paths\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(CFG.output_dir, CFG.summary_output), 'w') as f:\n",
    "            json.dump(combined_summary, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nComparison summary saved to {os.path.join(CFG.output_dir, CFG.summary_output)}\")\n",
    "        print(\"\\nDataset Comparison:\")\n",
    "        print(comparison_table)\n",
    "    else:\n",
    "        print(\"\\nCould not create comparison - one or both datasets failed to process\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T06:00:40.025989Z",
     "iopub.status.busy": "2025-05-21T06:00:40.025060Z",
     "iopub.status.idle": "2025-05-21T06:00:40.201641Z",
     "shell.execute_reply": "2025-05-21T06:00:40.200589Z",
     "shell.execute_reply.started": "2025-05-21T06:00:40.025961Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/florence2_results/ (stored 0%)\n",
      "  adding: kaggle/working/florence2_results/florence2_summary.json (deflated 57%)\n",
      "  adding: kaggle/working/florence2_results/dataset_comparison.png (deflated 22%)\n",
      "  adding: kaggle/working/florence2_results/inference_time_comparison.png (deflated 20%)\n",
      "  adding: kaggle/working/florence2_results/dataset_comparison.md (deflated 62%)\n",
      "  adding: kaggle/working/florence2_results/dataset_comparison.csv (deflated 30%)\n",
      "  adding: kaggle/working/florence2_results/florence2_form_results.json (deflated 82%)\n",
      "  adding: kaggle/working/florence2_results/florence2_receipt_results.json (deflated 66%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!zip -r handwritten_florence /kaggle/working/florence2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T07:24:47.083627Z",
     "iopub.status.busy": "2025-05-21T07:24:47.083393Z",
     "iopub.status.idle": "2025-05-21T07:33:12.114396Z",
     "shell.execute_reply": "2025-05-21T07:33:12.113816Z",
     "shell.execute_reply.started": "2025-05-21T07:24:47.083610Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model: meta-llama/Llama-3.2-11B-Vision-Instruct\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 07:24:56.959305: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747812297.139391      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747812297.191889      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/processing_auto.py:228: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e75588656064b7fb88f90d925a8f207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfbc870270d748f4a5d5d1131132fbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c4697c22834f85be0b9512a38bd626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/5.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8be7be966f4d68973f9c85ee9177e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2a99fedc684a8d92a585b7699f475e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b5e5cfb4a549bfbe264552e60df10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/5.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3268: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf52039f96c48bab2097153439b254e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/89.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954880f535aa433985fb8daa04933e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e37ee316e1d46a8b9afe1195812911d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eabc7e40fb0403495227b111ff1e613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1ea58ddf6640bd9ac8b348285796eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27bf6c959be402cac2858258701eb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6555ce5586d40609a5798a9748d7c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906cd40974c04ac48b6618c8bc4c80fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef063e72057746d08142334d3adcfa6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully, using device: cuda:0\n",
      "\n",
      "==================================================\n",
      "Processing Receipts dataset from /kaggle/input/handwritten-data-form-receipts/receipts/receipts\n",
      "==================================================\n",
      "Found 5 image-txt pairs\n",
      "\n",
      "Processing 1/5: image2\n",
      "Loaded image: /kaggle/input/handwritten-data-form-receipts/receipts/receipts/image2.jpg, size: (517, 748)\n",
      "Running inference with receipts prompt...\n",
      "\n",
      "OCR Result Preview:\n",
      "\n",
      "The receipt is from \"Dr. to A. Murphy\" and is dated January 9th, 1939. The store name is not visible. The items purchased are:\n",
      "\n",
      "* 2 weeks rent of cot...\n",
      "\n",
      "Ground Truth Preview:\n",
      "9th January 1939\n",
      "\n",
      "Mrs E. Martin\n",
      "No. 8\n",
      "Dr. to A. Murphy,\n",
      "\n",
      "2 weeks rent of cottage No. 8\n",
      "from 7th to 21st Jan 39 @ 35/-    3  10  0\n",
      "Sanitary fee        ...\n",
      "\n",
      "Calculating ROUGE scores...\n",
      "ROUGE-1: 0.5241\n",
      "ROUGE-2: 0.3636\n",
      "ROUGE-L: 0.4690\n",
      "\n",
      "Processing 2/5: image1\n",
      "Loaded image: /kaggle/input/handwritten-data-form-receipts/receipts/receipts/image1.jpg, size: (400, 532)\n",
      "Running inference with receipts prompt...\n",
      "\n",
      "OCR Result Preview:\n",
      "\n",
      "The receipt is from A Photographers Place, 133 Mercer Street, New York, N.Y. 10012. The date is August 5, 1988. The items purchased were Flora (3 pri...\n",
      "\n",
      "Ground Truth Preview:\n",
      "Date 19\n",
      "A PHOTOGRAPHER'S PLACE\n",
      "133 MERCER STREET\n",
      "NEW YORK, N.Y. 10012\n",
      "M No. Clerk Reg. No. \n",
      "\n",
      "1.8/5/98\n",
      "2\n",
      "3 FLORA 85\n",
      "4 (3) PRINTS 10\n",
      "5\n",
      "6\n",
      "7 TAX 7 85\n",
      "8\n",
      "9\n",
      "...\n",
      "\n",
      "Calculating ROUGE scores...\n",
      "ROUGE-1: 0.3538\n",
      "ROUGE-2: 0.1562\n",
      "ROUGE-L: 0.2462\n",
      "\n",
      "Processing 3/5: image4\n",
      "Loaded image: /kaggle/input/handwritten-data-form-receipts/receipts/receipts/image4.png, size: (850, 996)\n",
      "Running inference with receipts prompt...\n",
      "\n",
      "OCR Result Preview:\n",
      "\n",
      "The receipt is from Klinik ABC, located at 5, Jalan Anggerik, Taman Rekamas, 75450, Ayer Keroh, Melaka. The date of the receipt is 11/12/2020. The pa...\n",
      "\n",
      "Ground Truth Preview:\n",
      "MEDICAL BILL RECEIPT\n",
      "Receipt Number #14586\n",
      "Date 1011212020\n",
      "Name of Medical Institution: Clinic Yap\n",
      "Practitioner Name: Yap Chang Chui\n",
      "License Number: -...\n",
      "\n",
      "Calculating ROUGE scores...\n",
      "ROUGE-1: 0.5056\n",
      "ROUGE-2: 0.2500\n",
      "ROUGE-L: 0.2697\n",
      "\n",
      "Processing 4/5: image3\n",
      "Loaded image: /kaggle/input/handwritten-data-form-receipts/receipts/receipts/image3.png, size: (924, 601)\n",
      "Running inference with receipts prompt...\n",
      "\n",
      "OCR Result Preview:\n",
      "\n",
      "**Receipt Details**\n",
      "\n",
      "* **Store Name:** ABC Furniture Co.\n",
      "* **Date of Payment:** 41275 (no specific date provided)\n",
      "* **Items Purchased:**\n",
      "\t+ 2 single ...\n",
      "\n",
      "Ground Truth Preview:\n",
      "RECEIPT\n",
      "\n",
      "Payer: Annie Greens\n",
      "Payee: ABC Furniture Co\n",
      "Address: 5 Any Street\n",
      "Good Suburb\n",
      "Forrest Town X2204Y\n",
      "\n",
      "Address: 101 Tree Store\n",
      "Leafy Vale\n",
      "Forrest...\n",
      "\n",
      "Calculating ROUGE scores...\n",
      "ROUGE-1: 0.3488\n",
      "ROUGE-2: 0.2143\n",
      "ROUGE-L: 0.3023\n",
      "\n",
      "Processing 5/5: image5\n",
      "Loaded image: /kaggle/input/handwritten-data-form-receipts/receipts/receipts/image5.jpg, size: (720, 1203)\n",
      "Running inference with receipts prompt...\n",
      "\n",
      "OCR Result Preview:\n",
      "\n",
      "**Receipt Details**\n",
      "\n",
      "* **Store Name:** Williams' Book Store\n",
      "* **Date:** February 18, 1981\n",
      "* **Items Purchased:**\n",
      "\t+ \"Great Tastes of Chynese Cooking\"...\n",
      "\n",
      "Ground Truth Preview:\n",
      "WILLIAMS' BOOK STORE\n",
      "Established 1908\n",
      "708 SOUTH PACIFIC AVENUE\n",
      "San Pedro, Calif. 90731\n",
      "832-3631\n",
      "\n",
      "Date 2/21/1981\n",
      "\n",
      "Name Ellen Leonard\n",
      "Address 1351 W 15t...\n",
      "\n",
      "Calculating ROUGE scores...\n",
      "ROUGE-1: 0.2947\n",
      "ROUGE-2: 0.0860\n",
      "ROUGE-L: 0.2526\n",
      "\n",
      "Dataset processing complete. Results saved to ./llama32_results/llama32_receipt_results.json\n",
      "\n",
      "Average ROUGE Scores:\n",
      "  ROUGE-1: 0.4054\n",
      "  ROUGE-2: 0.2140\n",
      "  ROUGE-L: 0.3079\n",
      "Average inference time: 14.15 seconds\n",
      "\n",
      "==================================================\n",
      "Processing Forms dataset from /kaggle/input/handwritten-data-form-receipts/forms/forms\n",
      "==================================================\n",
      "Found 5 image-txt pairs\n",
      "\n",
      "Processing 1/5: image6\n",
      "Loaded image: /kaggle/input/handwritten-data-form-receipts/forms/forms/image6.jpg, size: (846, 1100)\n",
      "Running inference with forms prompt...\n",
      "\n",
      "OCR Result Preview:\n",
      "\n",
      "Here is the extracted text from the image:\n",
      "\n",
      "**Patient Details:**\n",
      "\n",
      "* Full Name: Virat Vaithar\n",
      "* Date of Birth: 28-05-1992\n",
      "* Gender: Male\n",
      "* Blood Group...\n",
      "\n",
      "Ground Truth Preview:\n",
      "Hospital Patient Registration Form\n",
      "Please fill out this form accurately. All information is confidential.\n",
      "\n",
      "Full Name: Virat Vaibhav\n",
      "Date of Birth (DD/...\n",
      "\n",
      "Calculating ROUGE scores...\n",
      "ROUGE-1: 0.6667\n",
      "ROUGE-2: 0.4398\n",
      "ROUGE-L: 0.5679\n",
      "\n",
      "Processing 2/5: image9\n",
      "Loaded image: /kaggle/input/handwritten-data-form-receipts/forms/forms/image9.jpg, size: (830, 1100)\n",
      "Running inference with forms prompt...\n",
      "\n",
      "OCR Result Preview:\n",
      "\n",
      "Here is the extracted text from the hospital patient registration form:\n",
      "\n",
      "**Patient Details:**\n",
      "\n",
      "* Full Name: Anant Rajput\n",
      "* Date of Birth: 28/05/2000\n",
      "...\n",
      "\n",
      "Ground Truth Preview:\n",
      "Hospital Patient Registration Form\n",
      "Please fill out this form accurately. All information is confidential.\n",
      "\n",
      "Full Name: Anant Rajput\n",
      "Date of Birth (DD/M...\n",
      "\n",
      "Calculating ROUGE scores...\n",
      "ROUGE-1: 0.6833\n",
      "ROUGE-2: 0.5042\n",
      "ROUGE-L: 0.6333\n",
      "\n",
      "Processing 3/5: image8\n",
      "Loaded image: /kaggle/input/handwritten-data-form-receipts/forms/forms/image8.jpg, size: (809, 1100)\n",
      "Running inference with forms prompt...\n",
      "\n",
      "OCR Result Preview:\n",
      "\n",
      "Here is the extracted text from the hospital patient registration form:\n",
      "\n",
      "**Patient Details:**\n",
      "\n",
      "* Full Name: Prachi Singh\n",
      "* Date of Birth: 28/10/1982\n",
      "...\n",
      "\n",
      "Ground Truth Preview:\n",
      "Hospital Patient Registration Form\n",
      "Please fill out this form accurately. All information is confidential.\n",
      "\n",
      "Full Name: Prachi Singh\n",
      "Date of Birth (DD/M...\n",
      "\n",
      "Calculating ROUGE scores...\n",
      "ROUGE-1: 0.6844\n",
      "ROUGE-2: 0.5471\n",
      "ROUGE-L: 0.6311\n",
      "\n",
      "Processing 4/5: image10\n",
      "Loaded image: /kaggle/input/handwritten-data-form-receipts/forms/forms/image10.jpg, size: (850, 1091)\n",
      "Running inference with forms prompt...\n",
      "\n",
      "OCR Result Preview:\n",
      "\n",
      "**Patient Details:**\n",
      "\n",
      "* **Full Name:** Sherya Jain\n",
      "* **Date of Birth:** 02/05/2012\n",
      "* **Gender:** M\n",
      "* **Blood Group:** A+\n",
      "* **Nationality:** Indian\n",
      "* ...\n",
      "\n",
      "Ground Truth Preview:\n",
      "Hospital Patient Registration Form\n",
      "Please fill out this form accurately. All information is confidential.\n",
      "\n",
      "Full Name: Shanya Jain\n",
      "Date of Birth (DD/MM...\n",
      "\n",
      "Calculating ROUGE scores...\n",
      "ROUGE-1: 0.6124\n",
      "ROUGE-2: 0.4638\n",
      "ROUGE-L: 0.5933\n",
      "\n",
      "Processing 5/5: image7\n",
      "Loaded image: /kaggle/input/handwritten-data-form-receipts/forms/forms/image7.jpg, size: (850, 1050)\n",
      "Running inference with forms prompt...\n",
      "\n",
      "OCR Result Preview:\n",
      "\n",
      "Here is the extracted text from the image:\n",
      "\n",
      "**Patient Details:**\n",
      "\n",
      "* Full Name: Kartik Bairwa\n",
      "* Date of Birth: 04-06-2014\n",
      "* Gender: Male\n",
      "* Blood Group...\n",
      "\n",
      "Ground Truth Preview:\n",
      "Hospital Patient Registration Form\n",
      "Please fill out this form accurately. All information is confidential.\n",
      "\n",
      "Full Name: Kartik Bairwa\n",
      "Date of Birth (DD/...\n",
      "\n",
      "Calculating ROUGE scores...\n",
      "ROUGE-1: 0.8340\n",
      "ROUGE-2: 0.7102\n",
      "ROUGE-L: 0.8097\n",
      "\n",
      "Dataset processing complete. Results saved to ./llama32_results/llama32_form_results.json\n",
      "\n",
      "Average ROUGE Scores:\n",
      "  ROUGE-1: 0.6962\n",
      "  ROUGE-2: 0.5330\n",
      "  ROUGE-L: 0.6471\n",
      "Average inference time: 33.83 seconds\n",
      "\n",
      "Comparison summary saved to ./llama32_results/llama32_summary.json\n",
      "\n",
      "Dataset Comparison:\n",
      "               Metric Receipts   Forms Better Dataset Difference %\n",
      "0             ROUGE-1   0.4054  0.6962          Forms       41.76%\n",
      "1             ROUGE-2   0.2140  0.5330          Forms       59.84%\n",
      "2             ROUGE-L   0.3079  0.6471          Forms       52.41%\n",
      "3  Inference Time (s)    14.15   33.83       Receipts       58.16%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from rouge_score import rouge_scorer\n",
    "import psutil\n",
    "import re\n",
    "\n",
    "# Configuration class\n",
    "class CFG:\n",
    "    # Model\n",
    "    model_name = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "    max_tokens = 256\n",
    "    temperature = 0.01\n",
    "    \n",
    "    # Dataset paths\n",
    "    receipt_dir = \"/kaggle/input/handwritten-data-form-receipts/receipts/receipts\"  # Folder with receipt images and txt files\n",
    "    form_dir = \"/kaggle/input/handwritten-data-form-receipts/forms/forms\"        # Folder with form images and txt files\n",
    "    \n",
    "    # Output data\n",
    "    output_dir = \"./llama32_results\"\n",
    "    receipt_output = \"llama32_receipt_results.json\"\n",
    "    form_output = \"llama32_form_results.json\"\n",
    "    summary_output = \"llama32_summary.json\"\n",
    "    \n",
    "    # Image formats to process\n",
    "    img_extensions = ['.jpg', '.jpeg', '.png', '.tif', '.tiff']\n",
    "    \n",
    "    # Prompts for different document types\n",
    "    receipt_prompt = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "<|image|>I have a receipt image. Please extract all the text from it including store name, date, items, prices, and total amount.\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "    \n",
    "    form_prompt = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "<|image|>I have a hospital registration form image. Please extract all text from it including patient details, emergency contacts, medical history (yes/no responses), and insurance information.\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage of the process\"\"\"\n",
    "    try:\n",
    "        process = psutil.Process(os.getpid())\n",
    "        memory_info = process.memory_info()\n",
    "        return {\n",
    "            'rss': memory_info.rss / (1024 * 1024),  # RSS in MB\n",
    "            'vms': memory_info.vms / (1024 * 1024)   # VMS in MB\n",
    "        }\n",
    "    except:\n",
    "        return {'rss': 0, 'vms': 0}\n",
    "\n",
    "def get_cuda_memory_usage():\n",
    "    \"\"\"Get CUDA memory usage using torch\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return {\n",
    "            'allocated': torch.cuda.memory_allocated() / (1024 * 1024),  # MB\n",
    "            'reserved': torch.cuda.memory_reserved() / (1024 * 1024),    # MB\n",
    "            'max_allocated': torch.cuda.max_memory_allocated() / (1024 * 1024)  # MB\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"Build Llama 3.2 model without triton dependencies\"\"\"\n",
    "    print(f'\\nLoading model: {CFG.model_name}\\n')\n",
    "    \n",
    "    try:\n",
    "        # Import without using BitsAndBytes quantization\n",
    "        from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "        \n",
    "        # Processor\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            CFG.model_name,\n",
    "            use_auth_token=os.environ.get(\"HF_TOKEN\")\n",
    "        )\n",
    "\n",
    "        # Model with half precision but no quantization\n",
    "        model = MllamaForConditionalGeneration.from_pretrained(\n",
    "            CFG.model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",  # Automatically decide which parts go on which devices\n",
    "            low_cpu_mem_usage=True,\n",
    "            use_auth_token=os.environ.get(\"HF_TOKEN\")\n",
    "        ).eval()\n",
    "        \n",
    "        print(f\"Model loaded successfully, using device: {next(model.parameters()).device}\")\n",
    "        return processor, model\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Llama model with MllamaForConditionalGeneration: {str(e)}\")\n",
    "        \n",
    "        # Try with AutoModelForCausalLM as fallback\n",
    "        try:\n",
    "            from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "            \n",
    "            processor = AutoProcessor.from_pretrained(\n",
    "                CFG.model_name,\n",
    "                use_auth_token=os.environ.get(\"HF_TOKEN\")\n",
    "            )\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                CFG.model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                low_cpu_mem_usage=True,\n",
    "                use_auth_token=os.environ.get(\"HF_TOKEN\")\n",
    "            ).eval()\n",
    "            \n",
    "            print(f\"Model loaded successfully with AutoModelForCausalLM, using device: {next(model.parameters()).device}\")\n",
    "            return processor, model\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"Error with AutoModelForCausalLM approach: {str(e2)}\")\n",
    "            raise RuntimeError(\"Failed to load Llama 3.2 model with both approaches\")\n",
    "\n",
    "def inference(image, model, processor, prompt):\n",
    "    \"\"\"Run inference with Llama 3.2\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Record metrics before inference\n",
    "    metrics_before = {\n",
    "        'memory': get_memory_usage(),\n",
    "        'cuda': get_cuda_memory_usage(),\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Prepare inputs\n",
    "        inputs = processor(\n",
    "            image,\n",
    "            prompt,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        # Run generation\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=CFG.max_tokens,\n",
    "                temperature=CFG.temperature\n",
    "            )\n",
    "\n",
    "        # Process output\n",
    "        ans = processor.decode(output[0])\n",
    "        result = ans.split('<|eot_id|><|start_header_id|>assistant<|end_header_id|>')[-1].split('<|eot_id|>')[0]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {str(e)}\")\n",
    "        result = f\"Error: {str(e)}\"\n",
    "    \n",
    "    # Record metrics after inference\n",
    "    metrics_after = {\n",
    "        'memory': get_memory_usage(),\n",
    "        'cuda': get_cuda_memory_usage(),\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "    \n",
    "    # Calculate runtime\n",
    "    inference_time = metrics_after['timestamp'] - metrics_before['timestamp']\n",
    "    \n",
    "    # Clear CUDA cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Return results and metrics\n",
    "    return {\n",
    "        'result': result,\n",
    "        'metrics': {\n",
    "            'before': metrics_before,\n",
    "            'after': metrics_after,\n",
    "            'inference_time': inference_time\n",
    "        }\n",
    "    }\n",
    "\n",
    "def load_annotation_from_txt(txt_path):\n",
    "    \"\"\"Load annotation from a text file\"\"\"\n",
    "    try:\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read().strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading annotation file {txt_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text for better ROUGE matching\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove non-alphanumeric chars except spaces\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    # Trim leading/trailing spaces\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def calculate_rouge_scores(predicted_text, reference_text):\n",
    "    \"\"\"Calculate ROUGE scores between prediction and reference\"\"\"\n",
    "    # Initialize the ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # Normalize texts\n",
    "    normalized_reference = normalize_text(reference_text)\n",
    "    normalized_prediction = normalize_text(predicted_text)\n",
    "    \n",
    "    # Skip if either text is empty\n",
    "    if not normalized_reference or not normalized_prediction:\n",
    "        return {\n",
    "            'rouge1': 0,\n",
    "            'rouge2': 0,\n",
    "            'rougeL': 0,\n",
    "            'debug': {\n",
    "                'reference_empty': not normalized_reference,\n",
    "                'prediction_empty': not normalized_prediction\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Calculate scores\n",
    "    scores = scorer.score(normalized_reference, normalized_prediction)\n",
    "    \n",
    "    # Debug info\n",
    "    debug_info = {\n",
    "        'reference_sample': normalized_reference[:100] + \"...\" if len(normalized_reference) > 100 else normalized_reference,\n",
    "        'prediction_sample': normalized_prediction[:100] + \"...\" if len(normalized_prediction) > 100 else normalized_prediction,\n",
    "        'reference_length': len(normalized_reference),\n",
    "        'prediction_length': len(normalized_prediction)\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'rouge1': scores['rouge1'].fmeasure,\n",
    "        'rouge2': scores['rouge2'].fmeasure,\n",
    "        'rougeL': scores['rougeL'].fmeasure,\n",
    "        'debug': debug_info\n",
    "    }\n",
    "\n",
    "def find_image_txt_pairs(directory):\n",
    "    \"\"\"Find matching image and txt files in a directory\"\"\"\n",
    "    # Get all files\n",
    "    all_files = os.listdir(directory)\n",
    "    \n",
    "    # Find image files\n",
    "    image_files = [f for f in all_files if any(f.lower().endswith(ext) for ext in CFG.img_extensions)]\n",
    "    \n",
    "    # Find matching txt files\n",
    "    pairs = []\n",
    "    for img_file in image_files:\n",
    "        base_name = os.path.splitext(img_file)[0]\n",
    "        txt_file = f\"{base_name}.txt\"\n",
    "        \n",
    "        if txt_file in all_files:\n",
    "            pairs.append({\n",
    "                'image': os.path.join(directory, img_file),\n",
    "                'txt': os.path.join(directory, txt_file),\n",
    "                'base_name': base_name\n",
    "            })\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def process_dataset(dataset_dir, output_file, model, processor, dataset_name):\n",
    "    \"\"\"Process a single dataset (receipts or forms)\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing {dataset_name} dataset from {dataset_dir}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Find image-txt pairs\n",
    "    pairs = find_image_txt_pairs(dataset_dir)\n",
    "    print(f\"Found {len(pairs)} image-txt pairs\")\n",
    "    \n",
    "    if len(pairs) == 0:\n",
    "        print(f\"No valid pairs found in {dataset_dir}\")\n",
    "        return None\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Select the appropriate prompt based on dataset type\n",
    "    if dataset_name.lower() == 'receipts':\n",
    "        prompt = CFG.receipt_prompt\n",
    "    else:  # forms\n",
    "        prompt = CFG.form_prompt\n",
    "    \n",
    "    # Process each pair\n",
    "    for i, pair in enumerate(pairs):\n",
    "        print(f\"\\nProcessing {i+1}/{len(pairs)}: {pair['base_name']}\")\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(pair['image'])\n",
    "            print(f\"Loaded image: {pair['image']}, size: {image.size}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {pair['image']}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Load annotation\n",
    "        ground_truth = load_annotation_from_txt(pair['txt'])\n",
    "        if not ground_truth:\n",
    "            print(f\"Empty or invalid annotation file: {pair['txt']}\")\n",
    "            continue\n",
    "        \n",
    "        # Run inference\n",
    "        print(f\"Running inference with {dataset_name.lower()} prompt...\")\n",
    "        ocr_result = inference(image=image, model=model, processor=processor, prompt=prompt)\n",
    "        \n",
    "        # Print OCR result preview\n",
    "        print(\"\\nOCR Result Preview:\")\n",
    "        preview = str(ocr_result['result'])[:150] + \"...\" if len(str(ocr_result['result'])) > 150 else str(ocr_result['result'])\n",
    "        print(preview)\n",
    "        \n",
    "        # Ground truth preview\n",
    "        print(\"\\nGround Truth Preview:\")\n",
    "        gt_preview = ground_truth[:150] + \"...\" if len(ground_truth) > 150 else ground_truth\n",
    "        print(gt_preview)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        print(\"\\nCalculating ROUGE scores...\")\n",
    "        rouge_scores = calculate_rouge_scores(ocr_result['result'], ground_truth)\n",
    "        \n",
    "        print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "        print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "        print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'filename': pair['base_name'],\n",
    "            'image_path': pair['image'],\n",
    "            'txt_path': pair['txt'],\n",
    "            'ocr_result': str(ocr_result['result']),\n",
    "            'ground_truth': ground_truth,\n",
    "            'rouge_scores': rouge_scores,\n",
    "            'inference_time': ocr_result['metrics']['inference_time']\n",
    "        })\n",
    "        \n",
    "        # Save intermediate results\n",
    "        os.makedirs(CFG.output_dir, exist_ok=True)\n",
    "        with open(os.path.join(CFG.output_dir, output_file), 'w') as f:\n",
    "            json.dump({\n",
    "                'model': CFG.model_name,\n",
    "                'dataset': dataset_name,\n",
    "                'results': results\n",
    "            }, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nDataset processing complete. Results saved to {os.path.join(CFG.output_dir, output_file)}\")\n",
    "    \n",
    "    # Calculate average scores\n",
    "    if results:\n",
    "        avg_rouge1 = sum(r['rouge_scores']['rouge1'] for r in results) / len(results)\n",
    "        avg_rouge2 = sum(r['rouge_scores']['rouge2'] for r in results) / len(results)\n",
    "        avg_rougeL = sum(r['rouge_scores']['rougeL'] for r in results) / len(results)\n",
    "        avg_time = sum(r['inference_time'] for r in results) / len(results)\n",
    "        \n",
    "        summary = {\n",
    "            'dataset': dataset_name,\n",
    "            'num_images': len(results),\n",
    "            'avg_rouge1': avg_rouge1,\n",
    "            'avg_rouge2': avg_rouge2,\n",
    "            'avg_rougeL': avg_rougeL,\n",
    "            'avg_inference_time': avg_time\n",
    "        }\n",
    "        \n",
    "        print(\"\\nAverage ROUGE Scores:\")\n",
    "        print(f\"  ROUGE-1: {avg_rouge1:.4f}\")\n",
    "        print(f\"  ROUGE-2: {avg_rouge2:.4f}\")\n",
    "        print(f\"  ROUGE-L: {avg_rougeL:.4f}\")\n",
    "        print(f\"Average inference time: {avg_time:.2f} seconds\")\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    return None\n",
    "\n",
    "def visualize_results(receipt_results, form_results):\n",
    "    \"\"\"Create visualizations comparing performance on both datasets\"\"\"\n",
    "    os.makedirs(CFG.output_dir, exist_ok=True)\n",
    "    \n",
    "    if not receipt_results or not form_results:\n",
    "        print(\"Not enough data to create visualizations\")\n",
    "        return\n",
    "    \n",
    "    # Extract metrics\n",
    "    datasets = ['Receipts', 'Forms']\n",
    "    rouge1_scores = [receipt_results['avg_rouge1'], form_results['avg_rouge1']]\n",
    "    rouge2_scores = [receipt_results['avg_rouge2'], form_results['avg_rouge2']]\n",
    "    rougeL_scores = [receipt_results['avg_rougeL'], form_results['avg_rougeL']]\n",
    "    inf_times = [receipt_results['avg_inference_time'], form_results['avg_inference_time']]\n",
    "    \n",
    "    # Create ROUGE scores comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = np.arange(len(datasets))\n",
    "    width = 0.25\n",
    "    \n",
    "    plt.bar(x - width, rouge1_scores, width, label='ROUGE-1', color='#2196F3')\n",
    "    plt.bar(x, rouge2_scores, width, label='ROUGE-2', color='#4CAF50')\n",
    "    plt.bar(x + width, rougeL_scores, width, label='ROUGE-L', color='#FFC107')\n",
    "    \n",
    "    plt.xlabel('Datasets')\n",
    "    plt.ylabel('ROUGE Score')\n",
    "    plt.title('Llama 3.2 Performance on Different Datasets')\n",
    "    plt.xticks(x, datasets)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for i, model_idx in enumerate(x):\n",
    "        plt.text(model_idx - width, rouge1_scores[i] + 0.02, f\"{rouge1_scores[i]:.3f}\", \n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "        plt.text(model_idx, rouge2_scores[i] + 0.02, f\"{rouge2_scores[i]:.3f}\", \n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "        plt.text(model_idx + width, rougeL_scores[i] + 0.02, f\"{rougeL_scores[i]:.3f}\", \n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CFG.output_dir, 'dataset_comparison.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Create inference time comparison\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(datasets, inf_times, color=['#2196F3', '#4CAF50'])\n",
    "    plt.xlabel('Datasets')\n",
    "    plt.ylabel('Average Inference Time (seconds)')\n",
    "    plt.title('Inference Time Comparison')\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for i, time in enumerate(inf_times):\n",
    "        plt.text(i, time + 0.1, f\"{time:.2f}s\", ha='center', va='bottom')\n",
    "    \n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CFG.output_dir, 'inference_time_comparison.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Return file paths for reporting\n",
    "    return {\n",
    "        'rouge_comparison': os.path.join(CFG.output_dir, 'dataset_comparison.png'),\n",
    "        'time_comparison': os.path.join(CFG.output_dir, 'inference_time_comparison.png')\n",
    "    }\n",
    "\n",
    "def create_comparison_table(receipt_summary, form_summary):\n",
    "    \"\"\"Create a comparison table for the two datasets\"\"\"\n",
    "    if not receipt_summary or not form_summary:\n",
    "        return None\n",
    "    \n",
    "    # Determine which dataset performs better for each metric\n",
    "    better_dataset = {\n",
    "        'rouge1': 'Receipts' if receipt_summary['avg_rouge1'] > form_summary['avg_rouge1'] else 'Forms',\n",
    "        'rouge2': 'Receipts' if receipt_summary['avg_rouge2'] > form_summary['avg_rouge2'] else 'Forms',\n",
    "        'rougeL': 'Receipts' if receipt_summary['avg_rougeL'] > form_summary['avg_rougeL'] else 'Forms',\n",
    "        'time': 'Receipts' if receipt_summary['avg_inference_time'] < form_summary['avg_inference_time'] else 'Forms'\n",
    "    }\n",
    "    \n",
    "    # Calculate percentage difference\n",
    "    diff_percent = {\n",
    "        'rouge1': abs(receipt_summary['avg_rouge1'] - form_summary['avg_rouge1']) / \n",
    "                 max(receipt_summary['avg_rouge1'], form_summary['avg_rouge1']) * 100,\n",
    "        'rouge2': abs(receipt_summary['avg_rouge2'] - form_summary['avg_rouge2']) / \n",
    "                 max(receipt_summary['avg_rouge2'], form_summary['avg_rouge2']) * 100,\n",
    "        'rougeL': abs(receipt_summary['avg_rougeL'] - form_summary['avg_rougeL']) / \n",
    "                 max(receipt_summary['avg_rougeL'], form_summary['avg_rougeL']) * 100,\n",
    "        'time': abs(receipt_summary['avg_inference_time'] - form_summary['avg_inference_time']) / \n",
    "               max(receipt_summary['avg_inference_time'], form_summary['avg_inference_time']) * 100\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame for the table\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame({\n",
    "        'Metric': ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'Inference Time (s)'],\n",
    "        'Receipts': [\n",
    "            f\"{receipt_summary['avg_rouge1']:.4f}\",\n",
    "            f\"{receipt_summary['avg_rouge2']:.4f}\",\n",
    "            f\"{receipt_summary['avg_rougeL']:.4f}\",\n",
    "            f\"{receipt_summary['avg_inference_time']:.2f}\"\n",
    "        ],\n",
    "        'Forms': [\n",
    "            f\"{form_summary['avg_rouge1']:.4f}\",\n",
    "            f\"{form_summary['avg_rouge2']:.4f}\",\n",
    "            f\"{form_summary['avg_rougeL']:.4f}\",\n",
    "            f\"{form_summary['avg_inference_time']:.2f}\"\n",
    "        ],\n",
    "        'Better Dataset': [\n",
    "            better_dataset['rouge1'],\n",
    "            better_dataset['rouge2'],\n",
    "            better_dataset['rougeL'],\n",
    "            better_dataset['time']\n",
    "        ],\n",
    "        'Difference %': [\n",
    "            f\"{diff_percent['rouge1']:.2f}%\",\n",
    "            f\"{diff_percent['rouge2']:.2f}%\",\n",
    "            f\"{diff_percent['rougeL']:.2f}%\",\n",
    "            f\"{diff_percent['time']:.2f}%\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_path = os.path.join(CFG.output_dir, 'dataset_comparison.csv')\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Also save as markdown table\n",
    "    with open(os.path.join(CFG.output_dir, 'dataset_comparison.md'), 'w') as f:\n",
    "        f.write(df.to_markdown(index=False))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # Create output directory\n",
    "    os.makedirs(CFG.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Build model once\n",
    "    processor, model = build_model()\n",
    "    \n",
    "    # Process receipt dataset\n",
    "    receipt_summary = process_dataset(\n",
    "        dataset_dir=CFG.receipt_dir,\n",
    "        output_file=CFG.receipt_output,\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        dataset_name=\"Receipts\"\n",
    "    )\n",
    "    \n",
    "    # Process form dataset\n",
    "    form_summary = process_dataset(\n",
    "        dataset_dir=CFG.form_dir,\n",
    "        output_file=CFG.form_output,\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        dataset_name=\"Forms\"\n",
    "    )\n",
    "    \n",
    "    # Create visualizations\n",
    "    if receipt_summary and form_summary:\n",
    "        chart_paths = visualize_results(receipt_summary, form_summary)\n",
    "        \n",
    "        # Create comparison table\n",
    "        comparison_table = create_comparison_table(receipt_summary, form_summary)\n",
    "        \n",
    "        # Save combined summary\n",
    "        combined_summary = {\n",
    "            'model': CFG.model_name,\n",
    "            'receipts': receipt_summary,\n",
    "            'forms': form_summary,\n",
    "            'charts': chart_paths\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(CFG.output_dir, CFG.summary_output), 'w') as f:\n",
    "            json.dump(combined_summary, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nComparison summary saved to {os.path.join(CFG.output_dir, CFG.summary_output)}\")\n",
    "        print(\"\\nDataset Comparison:\")\n",
    "        print(comparison_table)\n",
    "    else:\n",
    "        print(\"\\nCould not create comparison - one or both datasets failed to process\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T07:40:27.664159Z",
     "iopub.status.busy": "2025-05-21T07:40:27.663642Z",
     "iopub.status.idle": "2025-05-21T07:40:27.924437Z",
     "shell.execute_reply": "2025-05-21T07:40:27.923643Z",
     "shell.execute_reply.started": "2025-05-21T07:40:27.664134Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/llama32_results/ (stored 0%)\n",
      "  adding: kaggle/working/llama32_results/llama32_summary.json (deflated 54%)\n",
      "  adding: kaggle/working/llama32_results/dataset_comparison.csv (deflated 28%)\n",
      "  adding: kaggle/working/llama32_results/inference_time_comparison.png (deflated 19%)\n",
      "  adding: kaggle/working/llama32_results/dataset_comparison.png (deflated 22%)\n",
      "  adding: kaggle/working/llama32_results/dataset_comparison.md (deflated 60%)\n",
      "  adding: kaggle/working/llama32_results/llama32_form_results.json (deflated 82%)\n",
      "  adding: kaggle/working/llama32_results/llama32_receipt_results.json (deflated 66%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!zip -r llama2_handwritten /kaggle/working/llama32_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3753245,
     "sourceId": 6494011,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7474335,
     "sourceId": 11891525,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
