{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmB4zYA72gCm",
        "outputId": "b8a31dec-1232-4a72-ce27-13151769247d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 780 ms, sys: 169 ms, total: 949 ms\n",
            "Wall time: 2min 17s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install transformers==4.45.0\n",
        "!pip install bitsandbytes==0.44.1 accelerate\n",
        "! pip install einops flash_attn # florence 2\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ozffnTIACJz",
        "outputId": "b8afc065-04bf-4e10-f86e-63cb0efa2128"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=92ce8165ca0fef50091385fdc7cd09d10e8f9d36905041a17da2b41113c82532\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlg5kXXgFfOZ",
        "outputId": "55563b06-b3a6-4106-9218-191628da5b34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 50 images to process\n",
            "Loading Florence model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing image 1/50: 0000971160.png\n",
            "Converting L image to RGB\n",
            "Processing image 2/50: 0000989556.png\n",
            "Converting L image to RGB\n",
            "Processing image 3/50: 0000990274.png\n",
            "Converting L image to RGB\n",
            "Processing image 4/50: 0000999294.png\n",
            "Converting L image to RGB\n",
            "Processing image 5/50: 0001118259.png\n",
            "Converting L image to RGB\n",
            "Processing image 6/50: 0001123541.png\n",
            "Converting L image to RGB\n",
            "Processing image 7/50: 0001129658.png\n",
            "Converting L image to RGB\n",
            "Processing image 8/50: 0001209043.png\n",
            "Converting L image to RGB\n",
            "Processing image 9/50: 0001239897.png\n",
            "Converting L image to RGB\n",
            "Processing image 10/50: 0001438955.png\n",
            "Converting L image to RGB\n",
            "Processing image 11/50: 0001456787.png\n",
            "Converting L image to RGB\n",
            "Processing image 12/50: 0001463282.png\n",
            "Converting L image to RGB\n",
            "Processing image 13/50: 0001463448.png\n",
            "Converting L image to RGB\n",
            "Processing image 14/50: 0001476912.png\n",
            "Converting L image to RGB\n",
            "Processing image 15/50: 0001477983.png\n",
            "Converting L image to RGB\n",
            "Processing image 16/50: 0001485288.png\n",
            "Converting L image to RGB\n",
            "Processing image 17/50: 00040534.png\n",
            "Converting L image to RGB\n",
            "Processing image 18/50: 00070353.png\n",
            "Converting L image to RGB\n",
            "Processing image 19/50: 00093726.png\n",
            "Converting L image to RGB\n",
            "Processing image 20/50: 0011505151.png\n",
            "Converting L image to RGB\n",
            "Processing image 21/50: 0011838621.png\n",
            "Converting L image to RGB\n",
            "Processing image 22/50: 0011845203.png\n",
            "Converting L image to RGB\n",
            "Processing image 23/50: 0011856542.png\n",
            "Converting L image to RGB\n",
            "Processing image 24/50: 0011859695.png\n",
            "Converting L image to RGB\n",
            "Processing image 25/50: 0011899960.png\n",
            "Converting L image to RGB\n",
            "Processing image 26/50: 0011906503.png\n",
            "Converting L image to RGB\n",
            "Processing image 27/50: 0011973451.png\n",
            "Converting L image to RGB\n",
            "Processing image 28/50: 0011974919.png\n",
            "Converting L image to RGB\n",
            "Processing image 29/50: 0011976929.png\n",
            "Converting L image to RGB\n",
            "Processing image 30/50: 0012178355.png\n",
            "Converting L image to RGB\n",
            "Processing image 31/50: 0012199830.png\n",
            "Converting L image to RGB\n",
            "Processing image 32/50: 0012529284.png\n",
            "Converting L image to RGB\n",
            "Processing image 33/50: 0012529295.png\n",
            "Converting L image to RGB\n",
            "Processing image 34/50: 0012602424.png\n",
            "Converting L image to RGB\n",
            "Processing image 35/50: 0012947358.png\n",
            "Converting L image to RGB\n",
            "Processing image 36/50: 0013255595.png\n",
            "Converting L image to RGB\n",
            "Processing image 37/50: 00283813.png\n",
            "Converting L image to RGB\n",
            "Processing image 38/50: 0030031163.png\n",
            "Converting L image to RGB\n",
            "Processing image 39/50: 0030041455.png\n",
            "Converting L image to RGB\n",
            "Processing image 40/50: 0060000813.png\n",
            "Converting L image to RGB\n",
            "Processing image 41/50: 0060007216.png\n",
            "Converting L image to RGB\n",
            "Processing image 42/50: 0060024314.png\n",
            "Converting L image to RGB\n",
            "Processing image 43/50: 0060025670.png\n",
            "Converting L image to RGB\n",
            "Processing image 44/50: 0060029036.png\n",
            "Converting L image to RGB\n",
            "Processing image 45/50: 0060036622.png\n",
            "Converting L image to RGB\n",
            "Processing image 46/50: 0060068489.png\n",
            "Converting L image to RGB\n",
            "Processing image 47/50: 0060077689.png\n",
            "Converting L image to RGB\n",
            "Processing image 48/50: 0060080406.png\n",
            "Converting L image to RGB\n",
            "Processing image 49/50: 0060091229.png\n",
            "Converting L image to RGB\n",
            "Processing image 50/50: 0060094595.png\n",
            "Converting L image to RGB\n",
            "Results saved to florence_ocr_results.csv\n",
            "Loaded 50 OCR results\n",
            "Calculated ROUGE for 0000971160.png\n",
            "Calculated ROUGE for 0000989556.png\n",
            "Calculated ROUGE for 0000990274.png\n",
            "Calculated ROUGE for 0000999294.png\n",
            "Calculated ROUGE for 0001118259.png\n",
            "Calculated ROUGE for 0001123541.png\n",
            "Calculated ROUGE for 0001129658.png\n",
            "Calculated ROUGE for 0001209043.png\n",
            "Calculated ROUGE for 0001239897.png\n",
            "Calculated ROUGE for 0001438955.png\n",
            "Calculated ROUGE for 0001456787.png\n",
            "Calculated ROUGE for 0001463282.png\n",
            "Calculated ROUGE for 0001463448.png\n",
            "Calculated ROUGE for 0001476912.png\n",
            "Calculated ROUGE for 0001477983.png\n",
            "Calculated ROUGE for 0001485288.png\n",
            "Calculated ROUGE for 00040534.png\n",
            "Calculated ROUGE for 00070353.png\n",
            "Calculated ROUGE for 00093726.png\n",
            "Calculated ROUGE for 0011505151.png\n",
            "Calculated ROUGE for 0011838621.png\n",
            "Calculated ROUGE for 0011845203.png\n",
            "Calculated ROUGE for 0011856542.png\n",
            "Calculated ROUGE for 0011859695.png\n",
            "Calculated ROUGE for 0011899960.png\n",
            "Calculated ROUGE for 0011906503.png\n",
            "Calculated ROUGE for 0011973451.png\n",
            "Calculated ROUGE for 0011974919.png\n",
            "Calculated ROUGE for 0011976929.png\n",
            "Calculated ROUGE for 0012178355.png\n",
            "Calculated ROUGE for 0012199830.png\n",
            "Calculated ROUGE for 0012529284.png\n",
            "Calculated ROUGE for 0012529295.png\n",
            "Calculated ROUGE for 0012602424.png\n",
            "Calculated ROUGE for 0012947358.png\n",
            "Calculated ROUGE for 0013255595.png\n",
            "Calculated ROUGE for 00283813.png\n",
            "Calculated ROUGE for 0030031163.png\n",
            "Calculated ROUGE for 0030041455.png\n",
            "Calculated ROUGE for 0060000813.png\n",
            "Calculated ROUGE for 0060007216.png\n",
            "Calculated ROUGE for 0060024314.png\n",
            "Calculated ROUGE for 0060025670.png\n",
            "Calculated ROUGE for 0060029036.png\n",
            "Calculated ROUGE for 0060036622.png\n",
            "Calculated ROUGE for 0060068489.png\n",
            "Calculated ROUGE for 0060077689.png\n",
            "Calculated ROUGE for 0060080406.png\n",
            "Calculated ROUGE for 0060091229.png\n",
            "Calculated ROUGE for 0060094595.png\n",
            "\n",
            "Average ROUGE Scores:\n",
            "avg_rouge1_precision: 0.5907\n",
            "avg_rouge1_recall: 0.6141\n",
            "avg_rouge1_f1: 0.6014\n",
            "avg_rouge2_precision: 0.3419\n",
            "avg_rouge2_recall: 0.3550\n",
            "avg_rouge2_f1: 0.3480\n",
            "avg_rougeL_precision: 0.4783\n",
            "avg_rougeL_recall: 0.4975\n",
            "avg_rougeL_f1: 0.4872\n",
            "\n",
            "Top 5 images by ROUGE-L F1 score:\n",
            "1. 0030041455.png - ROUGE-L F1: 0.7692\n",
            "2. 00070353.png - ROUGE-L F1: 0.7431\n",
            "3. 0030031163.png - ROUGE-L F1: 0.7287\n",
            "4. 0011899960.png - ROUGE-L F1: 0.7190\n",
            "5. 0060091229.png - ROUGE-L F1: 0.7184\n",
            "\n",
            "Bottom 5 images by ROUGE-L F1 score:\n",
            "1. 0000989556.png - ROUGE-L F1: 0.2595\n",
            "2. 0001239897.png - ROUGE-L F1: 0.2572\n",
            "3. 0001438955.png - ROUGE-L F1: 0.2329\n",
            "4. 0000999294.png - ROUGE-L F1: 0.2157\n",
            "5. 0060094595.png - ROUGE-L F1: 0.2034\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from rouge_score import rouge_scorer\n",
        "import re\n",
        "import glob\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "import time\n",
        "import gc\n",
        "\n",
        "# Install rouge-score if needed\n",
        "# !pip install rouge-score\n",
        "\n",
        "# Configuration\n",
        "class CFG:\n",
        "    florece_model = \"microsoft/Florence-2-large\"\n",
        "    image_dir = '/content/drive/MyDrive/training_data/images'\n",
        "    annotation_dir= '//content/drive/MyDrive/training_data/annotations'  # Update this path\n",
        "    num_images = 50\n",
        "    output_csv = 'florence_ocr_results.csv'\n",
        "\n",
        "# Load model only once\n",
        "def build_model():\n",
        "    print('Loading Florence model...')\n",
        "    processor = AutoProcessor.from_pretrained(\n",
        "        CFG.florece_model,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Determine if CUDA is available and set device accordingly\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # If using CPU, use float32 instead of float16\n",
        "    dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        CFG.florece_model,\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=dtype,\n",
        "    ).to(device).eval()\n",
        "\n",
        "    return processor, model\n",
        "\n",
        "# Process single image\n",
        "def process_image(image_path, processor, model):\n",
        "    try:\n",
        "        # Load image\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        # Convert grayscale (1-channel) or any non-RGB images to RGB (3-channel)\n",
        "        if image.mode != 'RGB':\n",
        "            print(f\"Converting {image.mode} image to RGB\")\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        # Start timing\n",
        "        start_time = time.time()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "        # Run inference\n",
        "        inputs = processor(\n",
        "            text=\"<OCR>\",\n",
        "            images=image,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Convert inputs to the same dtype as model and move to device\n",
        "        inputs = {k: v.to(device=model.device, dtype=torch.float16 if k == \"pixel_values\" else v.dtype)\n",
        "                 for k, v in inputs.items()}\n",
        "\n",
        "        generated_ids = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            pixel_values=inputs[\"pixel_values\"],\n",
        "            max_new_tokens=1024,\n",
        "            num_beams=3\n",
        "        )\n",
        "\n",
        "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "        result = processor.post_process_generation(\n",
        "            generated_text,\n",
        "            task=\"<OCR>\",\n",
        "            image_size=(image.width, image.height)\n",
        "        )\n",
        "\n",
        "        # Get metrics\n",
        "        inference_time = time.time() - start_time\n",
        "        max_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)  # MB\n",
        "\n",
        "        # Clean up\n",
        "        del inputs, generated_ids\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        return result, inference_time, max_memory\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_path}: {str(e)}\")\n",
        "        return None, None, None\n",
        "\n",
        "# Main processing function for OCR\n",
        "def process_ocr():\n",
        "    # Get images\n",
        "    all_images = sorted([\n",
        "        os.path.join(CFG.image_dir, f)\n",
        "        for f in os.listdir(CFG.image_dir)\n",
        "        if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
        "    ])[:CFG.num_images]\n",
        "\n",
        "    print(f\"Found {len(all_images)} images to process\")\n",
        "\n",
        "    # Load model once\n",
        "    processor, model = build_model()\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for idx, img_path in enumerate(all_images, 1):\n",
        "        img_filename = os.path.basename(img_path)\n",
        "        print(f\"Processing image {idx}/{len(all_images)}: {img_filename}\")\n",
        "\n",
        "        result, inf_time, mem_usage = process_image(img_path, processor, model)\n",
        "\n",
        "        results.append({\n",
        "            'image_id': img_filename,\n",
        "            'ocr_text': str(result) if result else None,\n",
        "            'inference_time_sec': inf_time,\n",
        "            'gpu_memory_usage_mb': mem_usage\n",
        "        })\n",
        "\n",
        "    # Save results\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(CFG.output_csv, index=False)\n",
        "    print(f\"Results saved to {CFG.output_csv}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Function to extract all text from annotation file\n",
        "def extract_text_from_annotation(annotation_file):\n",
        "    try:\n",
        "        with open(annotation_file, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Your annotation format has a list of text entries\n",
        "        all_texts = []\n",
        "\n",
        "        # Extract text from each item in the list\n",
        "        if isinstance(data, list):\n",
        "            for item in data:\n",
        "                if 'text' in item:\n",
        "                    all_texts.append(item['text'])\n",
        "        # If the data is a dictionary with a list under a key like 'annotations'\n",
        "        elif isinstance(data, dict):\n",
        "            for key in data:\n",
        "                if isinstance(data[key], list):\n",
        "                    for item in data[key]:\n",
        "                        if isinstance(item, dict) and 'text' in item:\n",
        "                            all_texts.append(item['text'])\n",
        "\n",
        "        # Join all the text pieces\n",
        "        return \" \".join(all_texts)\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {annotation_file}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Function to clean text for ROUGE comparison\n",
        "def clean_text(text):\n",
        "    if text is None or text == \"None\":\n",
        "        return \"\"\n",
        "    # Remove extra whitespace, newlines and normalize\n",
        "    return re.sub(r'\\s+', ' ', str(text)).strip()\n",
        "\n",
        "# Main function to calculate ROUGE scores\n",
        "def calculate_rouge():\n",
        "    # Load OCR results\n",
        "    ocr_results_df = pd.read_csv(CFG.output_csv)\n",
        "    print(f\"Loaded {len(ocr_results_df)} OCR results\")\n",
        "\n",
        "    # Initialize ROUGE scorer\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    rouge_scores = []\n",
        "\n",
        "    # Process each image that has OCR results\n",
        "    for _, row in ocr_results_df.iterrows():\n",
        "        image_filename = row['image_id']\n",
        "        ocr_text = clean_text(row['ocr_text'])\n",
        "\n",
        "        # Skip if OCR failed\n",
        "        if not ocr_text:\n",
        "            print(f\"Skipping {image_filename} - No OCR text available\")\n",
        "            continue\n",
        "\n",
        "        # Find corresponding annotation file\n",
        "        base_name = os.path.splitext(image_filename)[0]\n",
        "        annotation_path = os.path.join(CFG.annotation_dir, f\"{base_name}.json\")\n",
        "\n",
        "        if not os.path.exists(annotation_path):\n",
        "            print(f\"No annotation found for {image_filename}\")\n",
        "            continue\n",
        "\n",
        "        # Extract text from annotation\n",
        "        ground_truth = extract_text_from_annotation(annotation_path)\n",
        "        ground_truth = clean_text(ground_truth)\n",
        "\n",
        "        if not ground_truth:\n",
        "            print(f\"Empty ground truth for {image_filename}\")\n",
        "            continue\n",
        "\n",
        "        # Calculate ROUGE scores\n",
        "        scores = scorer.score(ground_truth, ocr_text)\n",
        "\n",
        "        rouge_scores.append({\n",
        "            'image_file': image_filename,\n",
        "            'rouge1_precision': scores['rouge1'].precision,\n",
        "            'rouge1_recall': scores['rouge1'].recall,\n",
        "            'rouge1_f1': scores['rouge1'].fmeasure,\n",
        "            'rouge2_precision': scores['rouge2'].precision,\n",
        "            'rouge2_recall': scores['rouge2'].recall,\n",
        "            'rouge2_f1': scores['rouge2'].fmeasure,\n",
        "            'rougeL_precision': scores['rougeL'].precision,\n",
        "            'rougeL_recall': scores['rougeL'].recall,\n",
        "            'rougeL_f1': scores['rougeL'].fmeasure,\n",
        "            'ground_truth_length': len(ground_truth),\n",
        "            'ocr_text_length': len(ocr_text)\n",
        "        })\n",
        "\n",
        "        print(f\"Calculated ROUGE for {image_filename}\")\n",
        "\n",
        "    # Save ROUGE scores to JSON\n",
        "    with open('rouge_scores.json', 'w') as f:\n",
        "        json.dump(rouge_scores, f, indent=4)\n",
        "\n",
        "    # Calculate and print average scores\n",
        "    if rouge_scores:\n",
        "        avg_scores = {\n",
        "            'avg_rouge1_precision': np.mean([s['rouge1_precision'] for s in rouge_scores]),\n",
        "            'avg_rouge1_recall': np.mean([s['rouge1_recall'] for s in rouge_scores]),\n",
        "            'avg_rouge1_f1': np.mean([s['rouge1_f1'] for s in rouge_scores]),\n",
        "            'avg_rouge2_precision': np.mean([s['rouge2_precision'] for s in rouge_scores]),\n",
        "            'avg_rouge2_recall': np.mean([s['rouge2_recall'] for s in rouge_scores]),\n",
        "            'avg_rouge2_f1': np.mean([s['rouge2_f1'] for s in rouge_scores]),\n",
        "            'avg_rougeL_precision': np.mean([s['rougeL_precision'] for s in rouge_scores]),\n",
        "            'avg_rougeL_recall': np.mean([s['rougeL_recall'] for s in rouge_scores]),\n",
        "            'avg_rougeL_f1': np.mean([s['rougeL_f1'] for s in rouge_scores]),\n",
        "        }\n",
        "\n",
        "        print(\"\\nAverage ROUGE Scores:\")\n",
        "        for metric, value in avg_scores.items():\n",
        "            print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "        # Save summary\n",
        "        with open('rouge_scores_summary.json', 'w') as f:\n",
        "            json.dump(avg_scores, f, indent=4)\n",
        "\n",
        "        # Create sorted lists for best/worst performing images\n",
        "        sorted_by_f1 = sorted(rouge_scores, key=lambda x: x['rougeL_f1'], reverse=True)\n",
        "\n",
        "        print(\"\\nTop 5 images by ROUGE-L F1 score:\")\n",
        "        for i, score in enumerate(sorted_by_f1[:5]):\n",
        "            print(f\"{i+1}. {score['image_file']} - ROUGE-L F1: {score['rougeL_f1']:.4f}\")\n",
        "\n",
        "        print(\"\\nBottom 5 images by ROUGE-L F1 score:\")\n",
        "        for i, score in enumerate(sorted_by_f1[-5:]):\n",
        "            print(f\"{i+1}. {score['image_file']} - ROUGE-L F1: {score['rougeL_f1']:.4f}\")\n",
        "\n",
        "        return avg_scores\n",
        "    else:\n",
        "        print(\"No ROUGE scores calculated. Check your OCR results and annotation files.\")\n",
        "        return None\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # First, process images with OCR\n",
        "    process_ocr()\n",
        "\n",
        "    # Then calculate ROUGE scores\n",
        "    calculate_rouge()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
