{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:31:40.066749: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747758700.276322      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747758700.335600      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 images to process\n",
      "Loading Llama-3.2-11B-Vision-Instruct model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7cd08bf98e4462fb890f6d667ba9c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f635040d4947ceb156cc862b30279b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ebdafded4f4674b225e439bedc22bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/5.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f1a5b4f70943acaf275f94368b6166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28586772b4be4a44881bceb52e7e96f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e28a6ceb7ae41e6bb1b8aafbe3f1fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/5.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d40ddba4cc4515a4e2ab0fbf84db25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/89.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb943a275ac74afa851e45f145f48d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c800b3e0c2ef4042a22d828defad9bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9b9fed1c7941acad5e240c846705dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4fc0ccdd224d829a82a29af9f48822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a019ea6eb14ed281df46230e66ef89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ddd711938954596977dad8f20707a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47f32477a9044908e7f5f73f40c28e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3979c6807d48a0a8a070b59ae24977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model in 175.07 seconds.\n",
      "Initial GPU memory usage: 13066.53 MB\n",
      "Initial RAM usage: 7569.65 MB\n",
      "Processing image 1/50: 0000971160.png\n",
      "Converting L image to RGB\n",
      "Processing image 2/50: 0000989556.png\n",
      "Converting L image to RGB\n",
      "Processing image 3/50: 0000990274.png\n",
      "Converting L image to RGB\n",
      "Processing image 4/50: 0000999294.png\n",
      "Converting L image to RGB\n",
      "Processing image 5/50: 0001118259.png\n",
      "Converting L image to RGB\n",
      "Processing image 6/50: 0001123541.png\n",
      "Converting L image to RGB\n",
      "Processing image 7/50: 0001129658.png\n",
      "Converting L image to RGB\n",
      "Processing image 8/50: 0001209043.png\n",
      "Converting L image to RGB\n",
      "Processing image 9/50: 0001239897.png\n",
      "Converting L image to RGB\n",
      "Processing image 10/50: 0001438955.png\n",
      "Converting L image to RGB\n",
      "Processing image 11/50: 0001456787.png\n",
      "Converting L image to RGB\n",
      "Processing image 12/50: 0001463282.png\n",
      "Converting L image to RGB\n",
      "Processing image 13/50: 0001463448.png\n",
      "Converting L image to RGB\n",
      "Processing image 14/50: 0001476912.png\n",
      "Converting L image to RGB\n",
      "Processing image 15/50: 0001477983.png\n",
      "Converting L image to RGB\n",
      "Processing image 16/50: 0001485288.png\n",
      "Converting L image to RGB\n",
      "Processing image 17/50: 00040534.png\n",
      "Converting L image to RGB\n",
      "Processing image 18/50: 00070353.png\n",
      "Converting L image to RGB\n",
      "Processing image 19/50: 00093726.png\n",
      "Converting L image to RGB\n",
      "Processing image 20/50: 0011505151.png\n",
      "Converting L image to RGB\n",
      "Processing image 21/50: 0011838621.png\n",
      "Converting L image to RGB\n",
      "Processing image 22/50: 0011845203.png\n",
      "Converting L image to RGB\n",
      "Processing image 23/50: 0011856542.png\n",
      "Converting L image to RGB\n",
      "Processing image 24/50: 0011859695.png\n",
      "Converting L image to RGB\n",
      "Processing image 25/50: 0011899960.png\n",
      "Converting L image to RGB\n",
      "Processing image 26/50: 0011906503.png\n",
      "Converting L image to RGB\n",
      "Processing image 27/50: 0011973451.png\n",
      "Converting L image to RGB\n",
      "Processing image 28/50: 0011974919.png\n",
      "Converting L image to RGB\n",
      "Processing image 29/50: 0011976929.png\n",
      "Converting L image to RGB\n",
      "Processing image 30/50: 0012178355.png\n",
      "Converting L image to RGB\n",
      "Processing image 31/50: 0012199830.png\n",
      "Converting L image to RGB\n",
      "Processing image 32/50: 0012529284.png\n",
      "Converting L image to RGB\n",
      "Processing image 33/50: 0012529295.png\n",
      "Converting L image to RGB\n",
      "Processing image 34/50: 0012602424.png\n",
      "Converting L image to RGB\n",
      "Processing image 35/50: 0012947358.png\n",
      "Converting L image to RGB\n",
      "Processing image 36/50: 0013255595.png\n",
      "Converting L image to RGB\n",
      "Processing image 37/50: 00283813.png\n",
      "Converting L image to RGB\n",
      "Processing image 38/50: 0030031163.png\n",
      "Converting L image to RGB\n",
      "Processing image 39/50: 0030041455.png\n",
      "Converting L image to RGB\n",
      "Processing image 40/50: 0060000813.png\n",
      "Converting L image to RGB\n",
      "Processing image 41/50: 0060007216.png\n",
      "Converting L image to RGB\n",
      "Processing image 42/50: 0060024314.png\n",
      "Converting L image to RGB\n",
      "Processing image 43/50: 0060025670.png\n",
      "Converting L image to RGB\n",
      "Processing image 44/50: 0060029036.png\n",
      "Converting L image to RGB\n",
      "Processing image 45/50: 0060036622.png\n",
      "Converting L image to RGB\n",
      "Processing image 46/50: 0060068489.png\n",
      "Converting L image to RGB\n",
      "Processing image 47/50: 0060077689.png\n",
      "Converting L image to RGB\n",
      "Processing image 48/50: 0060080406.png\n",
      "Converting L image to RGB\n",
      "Processing image 49/50: 0060091229.png\n",
      "Converting L image to RGB\n",
      "Processing image 50/50: 0060094595.png\n",
      "Converting L image to RGB\n",
      "Results saved to llama_ocr_results.csv\n",
      "\n",
      "===== PERFORMANCE SUMMARY =====\n",
      "Total images processed: 50\n",
      "Successful OCR extractions: 50\n",
      "Model load time: 175.07 seconds\n",
      "Total execution time: 1175.19 seconds\n",
      "Average inference time: 23.11 seconds\n",
      "Peak GPU memory usage: 14083.88 MB\n",
      "Peak RAM usage: 7773.45 MB\n",
      "Loaded 50 OCR results\n",
      "Calculated ROUGE for 0000971160.png\n",
      "Calculated ROUGE for 0000989556.png\n",
      "Calculated ROUGE for 0000990274.png\n",
      "Calculated ROUGE for 0000999294.png\n",
      "Calculated ROUGE for 0001118259.png\n",
      "Calculated ROUGE for 0001123541.png\n",
      "Calculated ROUGE for 0001129658.png\n",
      "Calculated ROUGE for 0001209043.png\n",
      "Calculated ROUGE for 0001239897.png\n",
      "Calculated ROUGE for 0001438955.png\n",
      "Calculated ROUGE for 0001456787.png\n",
      "Calculated ROUGE for 0001463282.png\n",
      "Calculated ROUGE for 0001463448.png\n",
      "Calculated ROUGE for 0001476912.png\n",
      "Calculated ROUGE for 0001477983.png\n",
      "Calculated ROUGE for 0001485288.png\n",
      "Calculated ROUGE for 00040534.png\n",
      "Calculated ROUGE for 00070353.png\n",
      "Calculated ROUGE for 00093726.png\n",
      "Calculated ROUGE for 0011505151.png\n",
      "Calculated ROUGE for 0011838621.png\n",
      "Calculated ROUGE for 0011845203.png\n",
      "Calculated ROUGE for 0011856542.png\n",
      "Calculated ROUGE for 0011859695.png\n",
      "Calculated ROUGE for 0011899960.png\n",
      "Calculated ROUGE for 0011906503.png\n",
      "Calculated ROUGE for 0011973451.png\n",
      "Calculated ROUGE for 0011974919.png\n",
      "Calculated ROUGE for 0011976929.png\n",
      "Calculated ROUGE for 0012178355.png\n",
      "Calculated ROUGE for 0012199830.png\n",
      "Calculated ROUGE for 0012529284.png\n",
      "Calculated ROUGE for 0012529295.png\n",
      "Calculated ROUGE for 0012602424.png\n",
      "Calculated ROUGE for 0012947358.png\n",
      "Calculated ROUGE for 0013255595.png\n",
      "Calculated ROUGE for 00283813.png\n",
      "Calculated ROUGE for 0030031163.png\n",
      "Calculated ROUGE for 0030041455.png\n",
      "Calculated ROUGE for 0060000813.png\n",
      "Calculated ROUGE for 0060007216.png\n",
      "Calculated ROUGE for 0060024314.png\n",
      "Calculated ROUGE for 0060025670.png\n",
      "Calculated ROUGE for 0060029036.png\n",
      "Calculated ROUGE for 0060036622.png\n",
      "Calculated ROUGE for 0060068489.png\n",
      "Calculated ROUGE for 0060077689.png\n",
      "Calculated ROUGE for 0060080406.png\n",
      "Calculated ROUGE for 0060091229.png\n",
      "Calculated ROUGE for 0060094595.png\n",
      "\n",
      "Average ROUGE Scores for Llama-3.2-Vision:\n",
      "model: Llama-3.2-Vision\n",
      "avg_rouge1_precision: 0.0862\n",
      "avg_rouge1_recall: 0.0089\n",
      "avg_rouge1_f1: 0.0159\n",
      "avg_rouge2_precision: 0.0000\n",
      "avg_rouge2_recall: 0.0000\n",
      "avg_rouge2_f1: 0.0000\n",
      "avg_rougeL_precision: 0.0775\n",
      "avg_rougeL_recall: 0.0080\n",
      "avg_rougeL_f1: 0.0144\n",
      "num_images_processed: 50\n",
      "avg_inference_time_sec: 23.1088\n",
      "max_inference_time_sec: 24.0281\n",
      "min_inference_time_sec: 22.9604\n",
      "avg_total_time_sec: 23.1226\n",
      "max_total_time_sec: 24.1000\n",
      "min_total_time_sec: 22.9702\n",
      "avg_gpu_memory_usage_mb: 14083.8794\n",
      "max_gpu_memory_usage_mb: 14083.8794\n",
      "min_gpu_memory_usage_mb: 14083.8794\n",
      "avg_ram_usage_mb: 7773.4531\n",
      "max_ram_usage_mb: 7773.4531\n",
      "min_ram_usage_mb: 7773.4531\n",
      "\n",
      "Top 5 images by ROUGE-L F1 score:\n",
      "1. 0011899960.png - ROUGE-L F1: 0.0448\n",
      "2. 0030031163.png - ROUGE-L F1: 0.0438\n",
      "3. 0001239897.png - ROUGE-L F1: 0.0408\n",
      "4. 0011856542.png - ROUGE-L F1: 0.0374\n",
      "5. 00040534.png - ROUGE-L F1: 0.0343\n",
      "\n",
      "Bottom 5 images by ROUGE-L F1 score:\n",
      "1. 0030041455.png - ROUGE-L F1: 0.0000\n",
      "2. 0060000813.png - ROUGE-L F1: 0.0000\n",
      "3. 0060024314.png - ROUGE-L F1: 0.0000\n",
      "4. 0060025670.png - ROUGE-L F1: 0.0000\n",
      "5. 0060094595.png - ROUGE-L F1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers==4.45.0 bitsandbytes==0.44.1 accelerate\n",
    "!pip install -q rouge-score psutil\n",
    "!pip install -q triton flash-attn\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "import re\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "import sys\n",
    "import psutil\n",
    "\n",
    "# Install required packages if not already installed\n",
    "\n",
    "\n",
    "# Configuration\n",
    "class CFG:\n",
    "    llama_name = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "    image_dir = '/kaggle/input/form-ocr-dataset-1/training_data/images'\n",
    "    annotation_dir = '/kaggle/input/form-ocr-dataset-1/training_data/annotations'\n",
    "    num_images = 50\n",
    "    output_csv = 'llama_ocr_results.csv'\n",
    "    max_tokens = 256\n",
    "    temperature = 0.01\n",
    "    hf_token = \"\"  # Your HF token\n",
    "    use_4bit = False  # Set to False to avoid quantization issues\n",
    "    detailed_metrics = True  # Track detailed performance metrics\n",
    "\n",
    "# Configure Hugging Face token\n",
    "os.environ[\"HF_TOKEN\"] = CFG.hf_token\n",
    "\n",
    "# Function to get current memory usage\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    \n",
    "    metrics = {\n",
    "        'ram_usage_mb': memory_info.rss / (1024 * 1024),\n",
    "        'gpu_memory_mb': torch.cuda.max_memory_allocated() / (1024 * 1024) if torch.cuda.is_available() else 0,\n",
    "        'gpu_memory_reserved_mb': torch.cuda.max_memory_reserved() / (1024 * 1024) if torch.cuda.is_available() else 0\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Import required modules\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "# Load model only once\n",
    "def build_model():\n",
    "    print('Loading Llama-3.2-11B-Vision-Instruct model...')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        CFG.llama_name,\n",
    "        token=CFG.hf_token\n",
    "    )\n",
    "    \n",
    "    # Use 16-bit precision (more stable than 4-bit for this model)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        CFG.llama_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto',\n",
    "        token=CFG.hf_token\n",
    "    ).eval()\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Loaded model in {load_time:.2f} seconds.\")\n",
    "    \n",
    "    # Get initial memory metrics\n",
    "    memory_metrics = get_memory_usage()\n",
    "    print(f\"Initial GPU memory usage: {memory_metrics['gpu_memory_mb']:.2f} MB\")\n",
    "    print(f\"Initial RAM usage: {memory_metrics['ram_usage_mb']:.2f} MB\")\n",
    "    \n",
    "    return processor, model, load_time, memory_metrics\n",
    "\n",
    "# Process single image using Llama\n",
    "def process_image(image_path, processor, model):\n",
    "    try:\n",
    "        metrics = {\n",
    "            'load_time_sec': None,\n",
    "            'preprocessing_time_sec': None,\n",
    "            'inference_time_sec': None,\n",
    "            'total_time_sec': None,\n",
    "            'gpu_memory_usage_mb': None,\n",
    "            'ram_usage_mb': None,\n",
    "            'image_dimensions': None,\n",
    "            'success': False\n",
    "        }\n",
    "        \n",
    "        # Start total timing\n",
    "        total_start_time = time.time()\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        preprocess_start_time = time.time()\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(image_path)\n",
    "        metrics['image_dimensions'] = f\"{image.width}x{image.height}\"\n",
    "        \n",
    "        # Convert to RGB if not already\n",
    "        if image.mode != 'RGB':\n",
    "            print(f\"Converting {image.mode} image to RGB\")\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        # Record preprocessing time\n",
    "        metrics['preprocessing_time_sec'] = time.time() - preprocess_start_time\n",
    "        \n",
    "        # Reset CUDA memory stats if available\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        # Define prompt for OCR extraction\n",
    "        PROMPT = \"\"\"\n",
    "        <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "        \n",
    "        <|image|>Extract all the text from this image:\n",
    "        \n",
    "        <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Start inference timing\n",
    "        inference_start_time = time.time()\n",
    "        \n",
    "        # Process inputs - FIXED: only passing required parameters\n",
    "        model_inputs = processor(\n",
    "            images=image,\n",
    "            text=PROMPT,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        model_inputs = {k: v.to(model.device) for k, v in model_inputs.items() \n",
    "                       if k in ['input_ids', 'attention_mask']}  # Only use inputs the model expects\n",
    "        \n",
    "        # Generate output\n",
    "        output = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=CFG.max_tokens,\n",
    "            temperature=CFG.temperature\n",
    "        )\n",
    "        \n",
    "        # Record inference time\n",
    "        metrics['inference_time_sec'] = time.time() - inference_start_time\n",
    "        \n",
    "        # Decode output\n",
    "        full_text = processor.decode(output[0])\n",
    "        \n",
    "        # Extract assistant's response\n",
    "        if '<|start_header_id|>assistant<|end_header_id|>' in full_text:\n",
    "            extracted_text = full_text.split('<|start_header_id|>assistant<|end_header_id|>')[-1]\n",
    "            if '<|eot_id|>' in extracted_text:\n",
    "                extracted_text = extracted_text.split('<|eot_id|>')[0]\n",
    "        else:\n",
    "            # Fallback extraction\n",
    "            extracted_text = full_text.replace(PROMPT, \"\")\n",
    "        \n",
    "        # Get memory metrics\n",
    "        if CFG.detailed_metrics:\n",
    "            mem_metrics = get_memory_usage()\n",
    "            metrics['gpu_memory_usage_mb'] = mem_metrics['gpu_memory_mb']\n",
    "            metrics['ram_usage_mb'] = mem_metrics['ram_usage_mb']\n",
    "        \n",
    "        # Record total time\n",
    "        metrics['total_time_sec'] = time.time() - total_start_time\n",
    "        metrics['success'] = True\n",
    "        \n",
    "        # Clean up\n",
    "        del model_inputs, output\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        gc.collect()\n",
    "        \n",
    "        return extracted_text.strip(), metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {str(e)}\")\n",
    "        if 'metrics' in locals():\n",
    "            metrics['error'] = str(e)\n",
    "            return None, metrics\n",
    "        else:\n",
    "            return None, {'error': str(e), 'success': False}\n",
    "\n",
    "# Main processing function for OCR\n",
    "def process_ocr():\n",
    "    # Get images\n",
    "    all_images = sorted([\n",
    "        os.path.join(CFG.image_dir, f) \n",
    "        for f in os.listdir(CFG.image_dir) \n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "    ])[:CFG.num_images]\n",
    "    \n",
    "    print(f\"Found {len(all_images)} images to process\")\n",
    "    \n",
    "    # Load model once and track loading metrics\n",
    "    processor, model, model_load_time, initial_memory = build_model()\n",
    "    \n",
    "    results = []\n",
    "    overall_metrics = {\n",
    "        'model': CFG.llama_name,\n",
    "        'total_images': len(all_images),\n",
    "        'successful_images': 0,\n",
    "        'model_load_time_sec': model_load_time,\n",
    "        'total_execution_time_sec': 0,\n",
    "        'average_inference_time_sec': 0,\n",
    "        'initial_gpu_memory_mb': initial_memory['gpu_memory_mb'],\n",
    "        'peak_gpu_memory_mb': initial_memory['gpu_memory_mb'],\n",
    "        'initial_ram_memory_mb': initial_memory['ram_usage_mb'],\n",
    "        'peak_ram_memory_mb': initial_memory['ram_usage_mb']\n",
    "    }\n",
    "    \n",
    "    overall_start_time = time.time()\n",
    "    inference_times = []\n",
    "    \n",
    "    for idx, img_path in enumerate(all_images, 1):\n",
    "        img_filename = os.path.basename(img_path)\n",
    "        print(f\"Processing image {idx}/{len(all_images)}: {img_filename}\")\n",
    "        \n",
    "        ocr_text, metrics = process_image(img_path, processor, model)\n",
    "        \n",
    "        # Update metrics\n",
    "        if metrics.get('success', False):\n",
    "            overall_metrics['successful_images'] += 1\n",
    "            if metrics.get('inference_time_sec'):\n",
    "                inference_times.append(metrics['inference_time_sec'])\n",
    "            \n",
    "            if metrics.get('gpu_memory_usage_mb', 0) > overall_metrics['peak_gpu_memory_mb']:\n",
    "                overall_metrics['peak_gpu_memory_mb'] = metrics['gpu_memory_usage_mb']\n",
    "                \n",
    "            if metrics.get('ram_usage_mb', 0) > overall_metrics['peak_ram_memory_mb']:\n",
    "                overall_metrics['peak_ram_memory_mb'] = metrics['ram_usage_mb']\n",
    "        \n",
    "        # Store results\n",
    "        result_entry = {\n",
    "            'image_id': img_filename,\n",
    "            'ocr_text': ocr_text\n",
    "        }\n",
    "        \n",
    "        # Add detailed metrics if enabled\n",
    "        if CFG.detailed_metrics:\n",
    "            for key, value in metrics.items():\n",
    "                result_entry[key] = value\n",
    "        \n",
    "        results.append(result_entry)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_metrics['total_execution_time_sec'] = time.time() - overall_start_time\n",
    "    if inference_times:\n",
    "        overall_metrics['average_inference_time_sec'] = sum(inference_times) / len(inference_times)\n",
    "        overall_metrics['min_inference_time_sec'] = min(inference_times)\n",
    "        overall_metrics['max_inference_time_sec'] = max(inference_times)\n",
    "    \n",
    "    # Save detailed performance metrics\n",
    "    with open('llama_performance_metrics.json', 'w') as f:\n",
    "        json.dump(overall_metrics, f, indent=4)\n",
    "    \n",
    "    # Save results\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(CFG.output_csv, index=False)\n",
    "    print(f\"Results saved to {CFG.output_csv}\")\n",
    "    \n",
    "    # Print performance summary\n",
    "    print(\"\\n===== PERFORMANCE SUMMARY =====\")\n",
    "    print(f\"Total images processed: {overall_metrics['total_images']}\")\n",
    "    print(f\"Successful OCR extractions: {overall_metrics['successful_images']}\")\n",
    "    print(f\"Model load time: {overall_metrics['model_load_time_sec']:.2f} seconds\")\n",
    "    print(f\"Total execution time: {overall_metrics['total_execution_time_sec']:.2f} seconds\")\n",
    "    print(f\"Average inference time: {overall_metrics['average_inference_time_sec']:.2f} seconds\")\n",
    "    print(f\"Peak GPU memory usage: {overall_metrics['peak_gpu_memory_mb']:.2f} MB\")\n",
    "    print(f\"Peak RAM usage: {overall_metrics['peak_ram_memory_mb']:.2f} MB\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to extract all text from annotation file\n",
    "def extract_text_from_annotation(annotation_file):\n",
    "    try:\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Your annotation format has a list of text entries\n",
    "        all_texts = []\n",
    "        \n",
    "        # Extract text from each item in the list\n",
    "        if isinstance(data, list):\n",
    "            for item in data:\n",
    "                if 'text' in item:\n",
    "                    all_texts.append(item['text'])\n",
    "        # If the data is a dictionary with a list under a key like 'annotations'\n",
    "        elif isinstance(data, dict):\n",
    "            for key in data:\n",
    "                if isinstance(data[key], list):\n",
    "                    for item in data[key]:\n",
    "                        if isinstance(item, dict) and 'text' in item:\n",
    "                            all_texts.append(item['text'])\n",
    "        \n",
    "        # Join all the text pieces\n",
    "        return \" \".join(all_texts)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {annotation_file}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to clean text for ROUGE comparison\n",
    "def clean_text(text):\n",
    "    if text is None or text == \"None\":\n",
    "        return \"\"\n",
    "    # Remove extra whitespace, newlines and normalize\n",
    "    return re.sub(r'\\s+', ' ', str(text)).strip()\n",
    "\n",
    "# Main function to calculate ROUGE scores\n",
    "def calculate_rouge():\n",
    "    # Load OCR results\n",
    "    ocr_results_df = pd.read_csv(CFG.output_csv)\n",
    "    print(f\"Loaded {len(ocr_results_df)} OCR results\")\n",
    "    \n",
    "    # Initialize ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    rouge_scores = []\n",
    "    \n",
    "    # Process each image that has OCR results\n",
    "    for _, row in ocr_results_df.iterrows():\n",
    "        image_filename = row['image_id']\n",
    "        ocr_text = clean_text(row['ocr_text'])\n",
    "        \n",
    "        # Skip if OCR failed\n",
    "        if not ocr_text:\n",
    "            print(f\"Skipping {image_filename} - No OCR text available\")\n",
    "            continue\n",
    "        \n",
    "        # Find corresponding annotation file\n",
    "        base_name = os.path.splitext(image_filename)[0]\n",
    "        annotation_path = os.path.join(CFG.annotation_dir, f\"{base_name}.json\")\n",
    "        \n",
    "        if not os.path.exists(annotation_path):\n",
    "            print(f\"No annotation found for {image_filename}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract text from annotation\n",
    "        ground_truth = extract_text_from_annotation(annotation_path)\n",
    "        ground_truth = clean_text(ground_truth)\n",
    "        \n",
    "        if not ground_truth:\n",
    "            print(f\"Empty ground truth for {image_filename}\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        scores = scorer.score(ground_truth, ocr_text)\n",
    "        \n",
    "        # Add performance metrics if available\n",
    "        result_entry = {\n",
    "            'image_file': image_filename,\n",
    "            'rouge1_precision': scores['rouge1'].precision,\n",
    "            'rouge1_recall': scores['rouge1'].recall,\n",
    "            'rouge1_f1': scores['rouge1'].fmeasure,\n",
    "            'rouge2_precision': scores['rouge2'].precision,\n",
    "            'rouge2_recall': scores['rouge2'].recall,\n",
    "            'rouge2_f1': scores['rouge2'].fmeasure,\n",
    "            'rougeL_precision': scores['rougeL'].precision,\n",
    "            'rougeL_recall': scores['rougeL'].recall,\n",
    "            'rougeL_f1': scores['rougeL'].fmeasure,\n",
    "            'ground_truth_length': len(ground_truth),\n",
    "            'ocr_text_length': len(ocr_text)\n",
    "        }\n",
    "        \n",
    "        # Add performance metrics if available in the OCR results\n",
    "        for metric in ['inference_time_sec', 'total_time_sec', 'gpu_memory_usage_mb', 'ram_usage_mb']:\n",
    "            if metric in row:\n",
    "                result_entry[metric] = row[metric]\n",
    "        \n",
    "        rouge_scores.append(result_entry)\n",
    "        \n",
    "        print(f\"Calculated ROUGE for {image_filename}\")\n",
    "    \n",
    "    # Save ROUGE scores to JSON\n",
    "    with open('llama_rouge_scores.json', 'w') as f:\n",
    "        json.dump(rouge_scores, f, indent=4)\n",
    "    \n",
    "    # Calculate and print average scores\n",
    "    if rouge_scores:\n",
    "        avg_scores = {\n",
    "            'model': 'Llama-3.2-Vision',\n",
    "            'avg_rouge1_precision': np.mean([s['rouge1_precision'] for s in rouge_scores]),\n",
    "            'avg_rouge1_recall': np.mean([s['rouge1_recall'] for s in rouge_scores]),\n",
    "            'avg_rouge1_f1': np.mean([s['rouge1_f1'] for s in rouge_scores]),\n",
    "            'avg_rouge2_precision': np.mean([s['rouge2_precision'] for s in rouge_scores]),\n",
    "            'avg_rouge2_recall': np.mean([s['rouge2_recall'] for s in rouge_scores]),\n",
    "            'avg_rouge2_f1': np.mean([s['rouge2_f1'] for s in rouge_scores]),\n",
    "            'avg_rougeL_precision': np.mean([s['rougeL_precision'] for s in rouge_scores]),\n",
    "            'avg_rougeL_recall': np.mean([s['rougeL_recall'] for s in rouge_scores]),\n",
    "            'avg_rougeL_f1': np.mean([s['rougeL_f1'] for s in rouge_scores]),\n",
    "            'num_images_processed': len(rouge_scores)\n",
    "        }\n",
    "        \n",
    "        # Add performance metrics if available\n",
    "        for metric in ['inference_time_sec', 'total_time_sec', 'gpu_memory_usage_mb', 'ram_usage_mb']:\n",
    "            if any(metric in s for s in rouge_scores):\n",
    "                values = [s[metric] for s in rouge_scores if metric in s and s[metric] is not None]\n",
    "                if values:\n",
    "                    avg_scores[f'avg_{metric}'] = np.mean(values)\n",
    "                    avg_scores[f'max_{metric}'] = np.max(values)\n",
    "                    avg_scores[f'min_{metric}'] = np.min(values)\n",
    "        \n",
    "        print(\"\\nAverage ROUGE Scores for Llama-3.2-Vision:\")\n",
    "        for metric, value in avg_scores.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"{metric}: {value}\")\n",
    "        \n",
    "        # Save summary\n",
    "        with open('llama_rouge_scores_summary.json', 'w') as f:\n",
    "            json.dump(avg_scores, f, indent=4)\n",
    "        \n",
    "        # Create sorted lists for best/worst performing images\n",
    "        sorted_by_f1 = sorted(rouge_scores, key=lambda x: x['rougeL_f1'], reverse=True)\n",
    "        \n",
    "        print(\"\\nTop 5 images by ROUGE-L F1 score:\")\n",
    "        for i, score in enumerate(sorted_by_f1[:5]):\n",
    "            print(f\"{i+1}. {score['image_file']} - ROUGE-L F1: {score['rougeL_f1']:.4f}\")\n",
    "        \n",
    "        print(\"\\nBottom 5 images by ROUGE-L F1 score:\")\n",
    "        for i, score in enumerate(sorted_by_f1[-5:]):\n",
    "            print(f\"{i+1}. {score['image_file']} - ROUGE-L F1: {score['rougeL_f1']:.4f}\")\n",
    "        \n",
    "        return avg_scores\n",
    "    else:\n",
    "        print(\"No ROUGE scores calculated. Check your OCR results and annotation files.\")\n",
    "        return None\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Process images with OCR using Llama model\n",
    "    process_ocr()\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    calculate_rouge()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3753245,
     "sourceId": 6494011,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7466183,
     "sourceId": 11879951,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7468311,
     "sourceId": 11882839,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7470279,
     "sourceId": 11885614,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
