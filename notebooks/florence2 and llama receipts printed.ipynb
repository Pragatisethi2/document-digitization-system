{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T05:51:35.505136Z",
     "iopub.status.busy": "2025-05-21T05:51:35.504739Z",
     "iopub.status.idle": "2025-05-21T05:51:48.881259Z",
     "shell.execute_reply": "2025-05-21T05:51:48.880141Z",
     "shell.execute_reply.started": "2025-05-21T05:51:35.505114Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 217 ms, sys: 128 ms, total: 346 ms\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install transformers==4.45.0\n",
    "!pip install bitsandbytes==0.44.1 accelerate\n",
    "! pip install einops flash_attn # florence 2\n",
    "!pip install rouge_score\n",
    "\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T05:51:48.905076Z",
     "iopub.status.busy": "2025-05-21T05:51:48.904726Z",
     "iopub.status.idle": "2025-05-21T05:51:48.914718Z",
     "shell.execute_reply": "2025-05-21T05:51:48.913947Z",
     "shell.execute_reply.started": "2025-05-21T05:51:48.905057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = 'paste ur hf token'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FLORENCE 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-21T05:52:26.572313Z",
     "iopub.status.idle": "2025-05-21T05:52:26.572653Z",
     "shell.execute_reply": "2025-05-21T05:52:26.572487Z",
     "shell.execute_reply.started": "2025-05-21T05:52:26.572473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from rouge_score import rouge_scorer\n",
    "import re\n",
    "import glob\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Configuration\n",
    "class CFG:\n",
    "    # Model\n",
    "    florence_model = \"microsoft/Florence-2-base\"  # You can change to large if you have enough GPU memory\n",
    "    \n",
    "    # Input data\n",
    "    image_root = '/kaggle/input/ocr-receipts-text-detection/images'\n",
    "    annotation_file = '/kaggle/input/ocr-receipts-text-detection/annotations.xml'\n",
    "    \n",
    "    # Output data\n",
    "    output_csv = 'florence2_ocr_results.csv'\n",
    "    rouge_output = 'florence2_rouge_scores.json'\n",
    "    \n",
    "    # Process all images or limit to a specific number\n",
    "    num_images = 20  # Set to None to process all images\n",
    "\n",
    "# Load model\n",
    "def build_model():\n",
    "    print(f'\\nLoading Florence model: {CFG.florence_model}\\n')\n",
    "    \n",
    "    try:\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            CFG.florence_model, \n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        # Determine if CUDA is available and set device accordingly\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        # If using CPU, use float32 instead of float16\n",
    "        dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            CFG.florence_model,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=dtype,\n",
    "        ).to(device).eval()\n",
    "        \n",
    "        return processor, model\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Florence model: {str(e)}\")\n",
    "        print(\"\\nAttempting to load with different configuration...\")\n",
    "        \n",
    "        # Try with fewer parameters if memory issues\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            CFG.florence_model, \n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            CFG.florence_model,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            low_cpu_mem_usage=True\n",
    "        ).eval()\n",
    "        \n",
    "        return processor, model\n",
    "\n",
    "# Process single image - Keep original OCR processing\n",
    "def process_image(image_path, processor, model):\n",
    "    try:\n",
    "        print(f\"Processing image: {os.path.basename(image_path)}\")\n",
    "        \n",
    "        # Read image with OpenCV then convert to PIL\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Error: Could not read image {image_path}\")\n",
    "            return None, None, None\n",
    "            \n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(image_rgb)\n",
    "\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        # Run OCR inference with Florence model - using only \"<OCR>\" as requested\n",
    "        inputs = processor(\n",
    "            text=\"<OCR>\",  # Keep this exactly as \"<OCR>\" - do not modify\n",
    "            images=pil_image,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Move inputs to device\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device=device, dtype=torch.float16 if k == \"pixel_values\" else v.dtype)\n",
    "                 for k, v in inputs.items()}\n",
    "\n",
    "        # Generate text\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                pixel_values=inputs[\"pixel_values\"],\n",
    "                max_new_tokens=1024,\n",
    "                num_beams=3\n",
    "            )\n",
    "\n",
    "        # Decode text\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "        result = processor.post_process_generation(\n",
    "            generated_text,\n",
    "            task=\"<OCR>\",\n",
    "            image_size=(pil_image.width, pil_image.height)\n",
    "        )\n",
    "\n",
    "        # Get metrics\n",
    "        inference_time = time.time() - start_time\n",
    "        max_memory = torch.cuda.max_memory_allocated() / (1024 ** 2) if torch.cuda.is_available() else 0\n",
    "\n",
    "        # Clean up\n",
    "        del inputs, generated_ids\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        print(f\"Inference completed in {inference_time:.2f} seconds\")\n",
    "        \n",
    "        # Return OCR result, timing, and memory usage\n",
    "        return result, inference_time, max_memory\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n",
    "\n",
    "# Parse annotations from XML file - improved version\n",
    "def parse_annotations(xml_file):\n",
    "    \"\"\"Parse annotations from XML file with improved error handling\"\"\"\n",
    "    print(f\"Parsing annotations from: {xml_file}\")\n",
    "    try:\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        annotations = {}\n",
    "        \n",
    "        # Process each image\n",
    "        for image in root.findall('.//image'):\n",
    "            img_id = int(image.get('id'))\n",
    "            # Extract just the filename, handling both unix and windows paths\n",
    "            img_name = os.path.basename(image.get('name'))\n",
    "            \n",
    "            # Extract annotations for this image\n",
    "            boxes = []\n",
    "            for box in image.findall('.//box'):\n",
    "                label_type = box.get('label')\n",
    "                \n",
    "                # Find the text attribute\n",
    "                for attr in box.findall('.//attribute'):\n",
    "                    if attr.get('name') == 'text':\n",
    "                        text = attr.text\n",
    "                        if text is not None:\n",
    "                            boxes.append({\n",
    "                                'label': label_type,\n",
    "                                'text': text\n",
    "                            })\n",
    "                            break\n",
    "            \n",
    "            # Store annotations\n",
    "            annotations[img_name] = {\n",
    "                'image_id': img_id,\n",
    "                'boxes': boxes\n",
    "            }\n",
    "        \n",
    "        print(f\"Successfully parsed annotations for {len(annotations)} images\")\n",
    "        return annotations\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing XML file: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {}\n",
    "\n",
    "# Improved text cleaning that preserves important receipt characters\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text while preserving important receipt characters\"\"\"\n",
    "    if text is None or text == \"None\":\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string if needed\n",
    "    text = str(text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespace, newlines and normalize\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Preserve important receipt characters like $, ., /, %, etc.\n",
    "    text = re.sub(r'[^a-z0-9\\s$.,:;/%-@]', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Extract the ground truth text from annotation boxes - focus on text only\n",
    "def extract_ground_truth(image_filename, annotations):\n",
    "    \"\"\"Extract ground truth text from annotations - simple concatenation\"\"\"\n",
    "    if image_filename not in annotations:\n",
    "        return \"\"\n",
    "    \n",
    "    # Get all text from annotation boxes\n",
    "    all_text = ' '.join([box['text'] for box in annotations[image_filename]['boxes']])\n",
    "    return all_text\n",
    "\n",
    "# Main processing function for OCR\n",
    "def process_ocr():\n",
    "    # Load annotations\n",
    "    annotations = parse_annotations(CFG.annotation_file)\n",
    "    print(f\"Loaded annotations for {len(annotations)} images\")\n",
    "    \n",
    "    # Get image paths\n",
    "    all_images = sorted([\n",
    "        os.path.join(CFG.image_root, f)\n",
    "        for f in os.listdir(CFG.image_root)\n",
    "        if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff'))\n",
    "    ])\n",
    "    \n",
    "    # Limit number of images if specified\n",
    "    if CFG.num_images is not None:\n",
    "        all_images = all_images[:CFG.num_images]\n",
    "\n",
    "    print(f\"Found {len(all_images)} images to process\")\n",
    "\n",
    "    # Load model once\n",
    "    processor, model = build_model()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for idx, img_path in enumerate(all_images, 1):\n",
    "        img_filename = os.path.basename(img_path)\n",
    "        print(f\"\\nProcessing image {idx}/{len(all_images)}: {img_filename}\")\n",
    "\n",
    "        # Only process if we have annotations\n",
    "        if img_filename not in annotations:\n",
    "            print(f\"No annotations found for {img_filename}, skipping\")\n",
    "            continue\n",
    "\n",
    "        # Process the image\n",
    "        result, inf_time, mem_usage = process_image(img_path, processor, model)\n",
    "\n",
    "        # Skip if processing failed\n",
    "        if result is None:\n",
    "            print(f\"Failed to process {img_filename}, skipping\")\n",
    "            continue\n",
    "\n",
    "        # Get OCR text\n",
    "        if isinstance(result, dict) and '<OCR>' in result:\n",
    "            ocr_text = result['<OCR>']\n",
    "        else:\n",
    "            ocr_text = str(result)\n",
    "\n",
    "        # Preview OCR result\n",
    "        preview = ocr_text[:150] + \"...\" if len(ocr_text) > 150 else ocr_text\n",
    "        print(f\"OCR Result Preview: {preview}\")\n",
    "\n",
    "        # Store result\n",
    "        results.append({\n",
    "            'image_id': img_filename,\n",
    "            'ocr_text': ocr_text,\n",
    "            'inference_time_sec': inf_time,\n",
    "            'gpu_memory_usage_mb': mem_usage\n",
    "        })\n",
    "\n",
    "    # Save results\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(CFG.output_csv, index=False)\n",
    "    print(f\"Results saved to {CFG.output_csv}\")\n",
    "\n",
    "    return df, annotations\n",
    "\n",
    "# Calculate ROUGE scores between OCR results and ground truth - improved version\n",
    "def calculate_rouge(ocr_results_df, annotations):\n",
    "    print(\"\\nCalculating ROUGE scores...\")\n",
    "    \n",
    "    # Initialize ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    rouge_scores = []\n",
    "\n",
    "    # Process each image that has OCR results\n",
    "    for _, row in ocr_results_df.iterrows():\n",
    "        image_filename = row['image_id']\n",
    "        ocr_text = clean_text(row['ocr_text'])\n",
    "\n",
    "        # Skip if OCR failed\n",
    "        if not ocr_text:\n",
    "            print(f\"Skipping {image_filename} - No OCR text available\")\n",
    "            continue\n",
    "\n",
    "        # Get ground truth from annotations\n",
    "        ground_truth = extract_ground_truth(image_filename, annotations)\n",
    "        ground_truth = clean_text(ground_truth)\n",
    "\n",
    "        if not ground_truth:\n",
    "            print(f\"Empty ground truth for {image_filename}\")\n",
    "            continue\n",
    "\n",
    "        # Calculate ROUGE scores\n",
    "        scores = scorer.score(ground_truth, ocr_text)\n",
    "\n",
    "        # Calculate per-label scores\n",
    "        per_label_scores = {}\n",
    "        if image_filename in annotations:\n",
    "            for label_type in set(box['label'] for box in annotations[image_filename]['boxes']):\n",
    "                # Get text for this label only\n",
    "                label_text = ' '.join([\n",
    "                    box['text'] for box in annotations[image_filename]['boxes'] \n",
    "                    if box['label'] == label_type\n",
    "                ])\n",
    "                label_text = clean_text(label_text)\n",
    "                \n",
    "                # Skip if empty\n",
    "                if not label_text:\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate scores\n",
    "                label_scores = scorer.score(label_text, ocr_text)\n",
    "                per_label_scores[label_type] = {\n",
    "                    'rouge1': label_scores['rouge1'].fmeasure,\n",
    "                    'rouge2': label_scores['rouge2'].fmeasure,\n",
    "                    'rougeL': label_scores['rougeL'].fmeasure\n",
    "                }\n",
    "\n",
    "        # Store scores\n",
    "        rouge_scores.append({\n",
    "            'image_file': image_filename,\n",
    "            'rouge1_precision': scores['rouge1'].precision,\n",
    "            'rouge1_recall': scores['rouge1'].recall,\n",
    "            'rouge1_f1': scores['rouge1'].fmeasure,\n",
    "            'rouge2_precision': scores['rouge2'].precision,\n",
    "            'rouge2_recall': scores['rouge2'].recall,\n",
    "            'rouge2_f1': scores['rouge2'].fmeasure,\n",
    "            'rougeL_precision': scores['rougeL'].precision,\n",
    "            'rougeL_recall': scores['rougeL'].recall,\n",
    "            'rougeL_f1': scores['rougeL'].fmeasure,\n",
    "            'ground_truth_length': len(ground_truth),\n",
    "            'ocr_text_length': len(ocr_text),\n",
    "            'per_label_scores': per_label_scores,\n",
    "            'inference_time': row['inference_time_sec'],\n",
    "            'memory_usage': row['gpu_memory_usage_mb']\n",
    "        })\n",
    "\n",
    "        print(f\"Calculated ROUGE for {image_filename}: ROUGE-1 F1 = {scores['rouge1'].fmeasure:.4f}\")\n",
    "\n",
    "    # Save ROUGE scores to JSON\n",
    "    with open(CFG.rouge_output, 'w') as f:\n",
    "        json.dump(rouge_scores, f, indent=4)\n",
    "\n",
    "    # Calculate and print average scores\n",
    "    if rouge_scores:\n",
    "        avg_scores = {\n",
    "            'avg_rouge1_precision': np.mean([s['rouge1_precision'] for s in rouge_scores]),\n",
    "            'avg_rouge1_recall': np.mean([s['rouge1_recall'] for s in rouge_scores]),\n",
    "            'avg_rouge1_f1': np.mean([s['rouge1_f1'] for s in rouge_scores]),\n",
    "            'avg_rouge2_precision': np.mean([s['rouge2_precision'] for s in rouge_scores]),\n",
    "            'avg_rouge2_recall': np.mean([s['rouge2_recall'] for s in rouge_scores]),\n",
    "            'avg_rouge2_f1': np.mean([s['rouge2_f1'] for s in rouge_scores]),\n",
    "            'avg_rougeL_precision': np.mean([s['rougeL_precision'] for s in rouge_scores]),\n",
    "            'avg_rougeL_recall': np.mean([s['rougeL_recall'] for s in rouge_scores]),\n",
    "            'avg_rougeL_f1': np.mean([s['rougeL_f1'] for s in rouge_scores]),\n",
    "            'avg_inference_time': np.mean([s['inference_time'] for s in rouge_scores]),\n",
    "            'avg_memory_usage': np.mean([s['memory_usage'] for s in rouge_scores if s['memory_usage'] is not None])\n",
    "        }\n",
    "\n",
    "        print(\"\\nAverage ROUGE Scores:\")\n",
    "        for metric, value in avg_scores.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "        # Save summary\n",
    "        with open('florence2_rouge_summary.json', 'w') as f:\n",
    "            json.dump(avg_scores, f, indent=4)\n",
    "\n",
    "        # Create sorted lists for best/worst performing images\n",
    "        sorted_by_f1 = sorted(rouge_scores, key=lambda x: x['rougeL_f1'], reverse=True)\n",
    "\n",
    "        print(\"\\nTop 5 images by ROUGE-L F1 score:\")\n",
    "        for i, score in enumerate(sorted_by_f1[:5]):\n",
    "            print(f\"{i+1}. {score['image_file']} - ROUGE-L F1: {score['rougeL_f1']:.4f}\")\n",
    "\n",
    "        print(\"\\nBottom 5 images by ROUGE-L F1 score:\")\n",
    "        for i, score in enumerate(sorted_by_f1[-5:]):\n",
    "            print(f\"{i+1}. {score['image_file']} - ROUGE-L F1: {score['rougeL_f1']:.4f}\")\n",
    "\n",
    "        return avg_scores\n",
    "    else:\n",
    "        print(\"No ROUGE scores calculated. Check your OCR results and annotation files.\")\n",
    "        return None\n",
    "\n",
    "# Function to debug annotations for verification\n",
    "def debug_annotations(annotations, num_samples=3):\n",
    "    \"\"\"Print sample annotations to verify parsing is correct\"\"\"\n",
    "    print(\"\\nDebugging annotation parsing:\")\n",
    "    \n",
    "    if not annotations:\n",
    "        print(\"No annotations found!\")\n",
    "        return\n",
    "    \n",
    "    # Get a few sample images\n",
    "    sample_keys = list(annotations.keys())[:num_samples]\n",
    "    \n",
    "    for img_key in sample_keys:\n",
    "        annotation = annotations[img_key]\n",
    "        print(f\"\\nImage: {img_key}, ID: {annotation['image_id']}\")\n",
    "        print(f\"Number of annotation boxes: {len(annotation['boxes'])}\")\n",
    "        \n",
    "        # Print first few boxes\n",
    "        for i, box in enumerate(annotation['boxes'][:3]):\n",
    "            print(f\"  Box {i+1}:\")\n",
    "            print(f\"    Label: {box['label']}\")\n",
    "            print(f\"    Text: {box['text']}\")\n",
    "        \n",
    "        # Print ground truth as it would be used\n",
    "        ground_truth = extract_ground_truth(img_key, annotations)\n",
    "        print(f\"\\n  Full ground truth text ({len(ground_truth)} chars):\")\n",
    "        if len(ground_truth) > 100:\n",
    "            print(f\"    {ground_truth[:100]}...\")\n",
    "        else:\n",
    "            print(f\"    {ground_truth}\")\n",
    "        \n",
    "        # Print cleaned ground truth\n",
    "        cleaned_gt = clean_text(ground_truth)\n",
    "        print(f\"\\n  Cleaned ground truth text ({len(cleaned_gt)} chars):\")\n",
    "        if len(cleaned_gt) > 100:\n",
    "            print(f\"    {cleaned_gt[:100]}...\")\n",
    "        else:\n",
    "            print(f\"    {cleaned_gt}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Process images with OCR\n",
    "    ocr_results, annotations = process_ocr()\n",
    "    \n",
    "    # Debug annotations to verify parsing\n",
    "    debug_annotations(annotations)\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    calculate_rouge(ocr_results, annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLAMA 2 VISION INSTRUCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-21T05:52:26.578414Z",
     "iopub.status.idle": "2025-05-21T05:52:26.579020Z",
     "shell.execute_reply": "2025-05-21T05:52:26.578856Z",
     "shell.execute_reply.started": "2025-05-21T05:52:26.578841Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.45.0 bitsandbytes==0.44.1 accelerate\n",
    "!pip install -q rouge-score psutil\n",
    "!pip install -q triton flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-21T05:52:26.579925Z",
     "iopub.status.idle": "2025-05-21T05:52:26.580255Z",
     "shell.execute_reply": "2025-05-21T05:52:26.580078Z",
     "shell.execute_reply.started": "2025-05-21T05:52:26.580062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from rouge_score import rouge_scorer\n",
    "import psutil\n",
    "import re\n",
    "\n",
    "# Configuration class\n",
    "class CFG:\n",
    "    # Model\n",
    "    model_name = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "    max_tokens = 256\n",
    "    temperature = 0.01\n",
    "    \n",
    "    # Input data\n",
    "    image_root = '/kaggle/input/ocr-receipts-text-detection/images'\n",
    "    annotation_file = '/kaggle/input/ocr-receipts-text-detection/annotations.xml'\n",
    "    \n",
    "    # Output data\n",
    "    output_path = \"./llama32_results.json\"\n",
    "    \n",
    "    # Process all images\n",
    "    process_all = True\n",
    "    \n",
    "    # Prompts\n",
    "    ocr_prompt = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "<|image|>Extract text from Grocery Store bill Image:\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "    \n",
    "    filter_prompt = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "<|image|>Extract the total amount paid, items and the shop name from Grocery Store bill Image, use bullet points to organize the answer:\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage of the process\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    return {\n",
    "        'rss': memory_info.rss / (1024 * 1024),  # RSS in MB\n",
    "        'vms': memory_info.vms / (1024 * 1024)   # VMS in MB\n",
    "    }\n",
    "\n",
    "def get_cuda_memory_usage():\n",
    "    \"\"\"Get CUDA memory usage using torch\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return {\n",
    "            'allocated': torch.cuda.memory_allocated() / (1024 * 1024),  # MB\n",
    "            'reserved': torch.cuda.memory_reserved() / (1024 * 1024),    # MB\n",
    "            'max_allocated': torch.cuda.max_memory_allocated() / (1024 * 1024)  # MB\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def build_model(model_repo):\n",
    "    \"\"\"Build Llama 3.2 model without triton dependencies\"\"\"\n",
    "    print(f'\\nLoading model: {model_repo}\\n')\n",
    "    \n",
    "    try:\n",
    "        # Import without using BitsAndBytes quantization\n",
    "        from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "        \n",
    "        # Processor\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            model_repo,\n",
    "            use_auth_token=os.environ.get(\"HF_TOKEN\")\n",
    "        )\n",
    "\n",
    "        # Model with half precision but no quantization\n",
    "        model = MllamaForConditionalGeneration.from_pretrained(\n",
    "            model_repo,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",  # Automatically decide which parts go on which devices\n",
    "            low_cpu_mem_usage=True,\n",
    "            use_auth_token=os.environ.get(\"HF_TOKEN\")\n",
    "        ).eval()\n",
    "        \n",
    "        print(f\"Model loaded successfully, using device: {next(model.parameters()).device}\")\n",
    "        return processor, model\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Llama model: {str(e)}\")\n",
    "        raise  # Re-raise the exception since you want to use Llama 3.2 only\n",
    "\n",
    "def inference(prompt, image, model, processor):\n",
    "    \"\"\"Run inference with Llama 3.2\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Record metrics before inference\n",
    "    metrics_before = {\n",
    "        'memory': get_memory_usage(),\n",
    "        'cuda': get_cuda_memory_usage(),\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Prepare inputs\n",
    "        inputs = processor(\n",
    "            image,\n",
    "            prompt,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "\n",
    "        # Run generation\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=CFG.max_tokens,\n",
    "                temperature=CFG.temperature\n",
    "            )\n",
    "\n",
    "        # Process output\n",
    "        ans = processor.decode(output[0])\n",
    "        result = ans.split('<|eot_id|><|start_header_id|>assistant<|end_header_id|>')[-1].split('<|eot_id|>')[0]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during inference: {str(e)}\")\n",
    "        result = f\"Error: {str(e)}\"\n",
    "    \n",
    "    # Record metrics after inference\n",
    "    metrics_after = {\n",
    "        'memory': get_memory_usage(),\n",
    "        'cuda': get_cuda_memory_usage(),\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "    \n",
    "    # Calculate runtime\n",
    "    inference_time = metrics_after['timestamp'] - metrics_before['timestamp']\n",
    "    \n",
    "    # Clear CUDA cache\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Return results and metrics\n",
    "    return {\n",
    "        'result': result,\n",
    "        'metrics': {\n",
    "            'before': metrics_before,\n",
    "            'after': metrics_after,\n",
    "            'inference_time': inference_time\n",
    "        }\n",
    "    }\n",
    "\n",
    "def parse_annotations(xml_file):\n",
    "    \"\"\"Parse annotations from XML file\"\"\"\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    annotations = {}\n",
    "    \n",
    "    # Process each image\n",
    "    for image in root.findall('.//image'):\n",
    "        img_id = int(image.get('id'))\n",
    "        img_name = image.get('name').split('/')[-1]\n",
    "        \n",
    "        # Extract annotations for this image\n",
    "        boxes = []\n",
    "        for box in image.findall('.//box'):\n",
    "            label_type = box.get('label')\n",
    "            text = box.find('.//attribute[@name=\"text\"]')\n",
    "            \n",
    "            if text is not None and text.text is not None:\n",
    "                boxes.append({\n",
    "                    'label': label_type,\n",
    "                    'text': text.text\n",
    "                })\n",
    "        \n",
    "        # Store annotations\n",
    "        annotations[img_id] = {\n",
    "            'filename': img_name,\n",
    "            'boxes': boxes\n",
    "        }\n",
    "    \n",
    "    return annotations\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text for better ROUGE matching\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Replace multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove non-alphanumeric chars except spaces\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    # Trim leading/trailing spaces\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def calculate_rouge_scores(predicted_text, reference_boxes):\n",
    "    \"\"\"Calculate ROUGE scores between prediction and references\"\"\"\n",
    "    # Initialize the ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # Combine all reference texts\n",
    "    all_text = ' '.join([box['text'] for box in reference_boxes])\n",
    "    normalized_reference = normalize_text(all_text)\n",
    "    \n",
    "    # Ensure predicted text is a string\n",
    "    predicted = str(predicted_text)\n",
    "    normalized_prediction = normalize_text(predicted)\n",
    "    \n",
    "    # Calculate overall ROUGE score with normalized text\n",
    "    overall_scores = scorer.score(normalized_reference, normalized_prediction)\n",
    "    \n",
    "    # Create per-label normalized texts and scores\n",
    "    label_scores = {}\n",
    "    references_by_label = {}\n",
    "    \n",
    "    for box in reference_boxes:\n",
    "        label = box['label']\n",
    "        if label not in references_by_label:\n",
    "            references_by_label[label] = []\n",
    "            \n",
    "        references_by_label[label].append(box['text'])\n",
    "    \n",
    "    for label, texts in references_by_label.items():\n",
    "        label_text = ' '.join(texts)\n",
    "        normalized_label_text = normalize_text(label_text)\n",
    "        label_scores[label] = scorer.score(normalized_label_text, normalized_prediction)\n",
    "    \n",
    "    # Debug info to understand why scores might be low\n",
    "    debug_info = {\n",
    "        'total_reference_chars': len(normalized_reference),\n",
    "        'total_prediction_chars': len(normalized_prediction),\n",
    "        'normalized_reference_sample': normalized_reference[:100] + \"...\" if len(normalized_reference) > 100 else normalized_reference,\n",
    "        'normalized_prediction_sample': normalized_prediction[:100] + \"...\" if len(normalized_prediction) > 100 else normalized_prediction,\n",
    "        'labels_found': list(references_by_label.keys())\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'rouge1': overall_scores['rouge1'].fmeasure,\n",
    "        'rouge2': overall_scores['rouge2'].fmeasure,\n",
    "        'rougeL': overall_scores['rougeL'].fmeasure,\n",
    "        'per_label': {label: {\n",
    "            'rouge1': scores['rouge1'].fmeasure,\n",
    "            'rouge2': scores['rouge2'].fmeasure,\n",
    "            'rougeL': scores['rougeL'].fmeasure\n",
    "        } for label, scores in label_scores.items()},\n",
    "        'debug': debug_info\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Load annotations\n",
    "    annotations = parse_annotations(CFG.annotation_file)\n",
    "    print(f\"Loaded annotations for {len(annotations)} images\")\n",
    "    \n",
    "    # Build model\n",
    "    processor, model = build_model(CFG.model_name)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process images - either all or just first 3\n",
    "    if CFG.process_all:\n",
    "        test_img_ids = list(annotations.keys())\n",
    "    else:\n",
    "        test_img_ids = list(annotations.keys())[:3]\n",
    "    \n",
    "    print(f\"Processing {len(test_img_ids)} images\")\n",
    "    \n",
    "    # Process each image\n",
    "    for img_id in test_img_ids:\n",
    "        annotation = annotations[img_id]\n",
    "        print(f\"\\nProcessing image {img_id}: {annotation['filename']}\")\n",
    "        \n",
    "        # Display annotation summary\n",
    "        print(f\"Annotations: {len(annotation['boxes'])} boxes\")\n",
    "        for i, box in enumerate(annotation['boxes']):\n",
    "            if i < 3 or i == len(annotation['boxes']) - 1:  # Show first 3 and last annotation\n",
    "                print(f\"  {box['label']}: {box['text']}\")\n",
    "            elif i == 3:\n",
    "                print(f\"  ... ({len(annotation['boxes']) - 4} more) ...\")\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(CFG.image_root, annotation['filename'])\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            print(f\"Image {img_path} not found. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(image_rgb)\n",
    "        \n",
    "        # Run OCR inference\n",
    "        print(\"\\nRunning OCR extraction...\")\n",
    "        ocr_result = inference(prompt=CFG.ocr_prompt, image=pil_image, model=model, processor=processor)\n",
    "        \n",
    "        # Print OCR result preview\n",
    "        print(\"\\nOCR Result Preview:\")\n",
    "        preview = ocr_result['result'][:150] + \"...\" if len(ocr_result['result']) > 150 else ocr_result['result']\n",
    "        print(preview)\n",
    "        \n",
    "        # Run filtered OCR inference\n",
    "        print(\"\\nRunning filtered extraction...\")\n",
    "        filter_result = inference(prompt=CFG.filter_prompt, image=pil_image, model=model, processor=processor)\n",
    "        \n",
    "        # Print filtered result preview\n",
    "        print(\"\\nFiltered Result Preview:\")\n",
    "        filter_preview = filter_result['result'][:150] + \"...\" if len(filter_result['result']) > 150 else filter_result['result']\n",
    "        print(filter_preview)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        print(\"\\nCalculating ROUGE scores...\")\n",
    "        rouge_scores = calculate_rouge_scores(ocr_result['result'], annotation['boxes'])\n",
    "        \n",
    "        print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "        print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "        print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "        \n",
    "        # Print per-label scores\n",
    "        print(\"\\nPer-label ROUGE-1 scores:\")\n",
    "        for label, scores in rouge_scores['per_label'].items():\n",
    "            print(f\"  {label}: {scores['rouge1']:.4f}\")\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'image_id': img_id,\n",
    "            'filename': annotation['filename'],\n",
    "            'ocr_result': ocr_result['result'],\n",
    "            'filter_result': filter_result['result'],\n",
    "            'ground_truth': [box for box in annotation['boxes']],\n",
    "            'rouge_scores': rouge_scores,\n",
    "            'ocr_metrics': ocr_result['metrics'],\n",
    "            'filter_metrics': filter_result['metrics']\n",
    "        })\n",
    "        \n",
    "        # Save intermediate results\n",
    "        with open(CFG.output_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'model': CFG.model_name,\n",
    "                'results': results\n",
    "            }, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nEvaluation complete. Results saved to {CFG.output_path}\")\n",
    "    \n",
    "    # Calculate and display average scores\n",
    "    if results:\n",
    "        avg_rouge1 = sum(r['rouge_scores']['rouge1'] for r in results) / len(results)\n",
    "        avg_rouge2 = sum(r['rouge_scores']['rouge2'] for r in results) / len(results)\n",
    "        avg_rougeL = sum(r['rouge_scores']['rougeL'] for r in results) / len(results)\n",
    "        \n",
    "        # Calculate average inference time\n",
    "        avg_inference_time = sum(r['ocr_metrics']['inference_time'] for r in results) / len(results)\n",
    "        \n",
    "        # Calculate average memory usage if data is available\n",
    "        cuda_metrics = [r['ocr_metrics']['after']['cuda'] for r in results if r['ocr_metrics']['after']['cuda'] is not None]\n",
    "        if cuda_metrics:\n",
    "            avg_allocated_memory = sum(m['allocated'] for m in cuda_metrics) / len(cuda_metrics)\n",
    "            avg_reserved_memory = sum(m['reserved'] for m in cuda_metrics) / len(cuda_metrics)\n",
    "            print(f\"\\nAverage CUDA memory allocated: {avg_allocated_memory:.2f} MB\")\n",
    "            print(f\"Average CUDA memory reserved: {avg_reserved_memory:.2f} MB\")\n",
    "        \n",
    "        # Print average scores\n",
    "        print(\"\\nAverage ROUGE Scores:\")\n",
    "        print(f\"  ROUGE-1: {avg_rouge1:.4f}\")\n",
    "        print(f\"  ROUGE-2: {avg_rouge2:.4f}\")\n",
    "        print(f\"  ROUGE-L: {avg_rougeL:.4f}\")\n",
    "        print(f\"  Average inference time: {avg_inference_time:.2f} seconds\")\n",
    "        \n",
    "        # Per-label average scores\n",
    "        print(\"\\nAverage ROUGE-1 per label:\")\n",
    "        label_types = set()\n",
    "        for r in results:\n",
    "            label_types.update(r['rouge_scores']['per_label'].keys())\n",
    "        \n",
    "        for label in label_types:\n",
    "            scores = [r['rouge_scores']['per_label'][label]['rouge1'] \n",
    "                    for r in results \n",
    "                    if label in r['rouge_scores']['per_label']]\n",
    "            if scores:\n",
    "                avg_score = sum(scores) / len(scores)\n",
    "                print(f\"  {label}: {avg_score:.4f} (from {len(scores)} images)\")\n",
    "        \n",
    "        # Create summary output\n",
    "        summary = {\n",
    "            'model': CFG.model_name,\n",
    "            'num_images_processed': len(results),\n",
    "            'avg_scores': {\n",
    "                'rouge1': avg_rouge1,\n",
    "                'rouge2': avg_rouge2,\n",
    "                'rougeL': avg_rougeL\n",
    "            },\n",
    "            'avg_inference_time': avg_inference_time,\n",
    "            'per_label_avg_scores': {}\n",
    "        }\n",
    "        \n",
    "        # Add per-label stats to summary\n",
    "        for label in label_types:\n",
    "            scores = [r['rouge_scores']['per_label'][label]['rouge1'] \n",
    "                    for r in results \n",
    "                    if label in r['rouge_scores']['per_label']]\n",
    "            if scores:\n",
    "                summary['per_label_avg_scores'][label] = sum(scores) / len(scores)\n",
    "        \n",
    "        # Save summary\n",
    "        with open(\"llama32_summary.json\", 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nSummary saved to llama32_summary.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3753245,
     "sourceId": 6494011,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7474335,
     "sourceId": 11891525,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
